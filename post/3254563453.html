<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>人工智能 | 白のBlog</title><meta name="author" content="白"><meta name="copyright" content="白"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="人工智能的三大学派人工智能主要学派分为符号主义、连接主义、行为主义。  符号主义（Symbolicism）学派：认为人工智能源于数理逻辑。该学派将数学严格公理化，从公理出发，由逻辑推理得到引理，定理，推论。  连接主义（Connectionism）学派：认为人工智能源于仿生学，特别是对人脑模型的研究。  行为主义（Actionism）学派：来源于控制论及“感知—动作”型控制系统。该学派认为智能取决">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能">
<meta property="og:url" content="https://ran.ranruo.xyz/post/3254563453.html">
<meta property="og:site_name" content="白のBlog">
<meta property="og:description" content="人工智能的三大学派人工智能主要学派分为符号主义、连接主义、行为主义。  符号主义（Symbolicism）学派：认为人工智能源于数理逻辑。该学派将数学严格公理化，从公理出发，由逻辑推理得到引理，定理，推论。  连接主义（Connectionism）学派：认为人工智能源于仿生学，特别是对人脑模型的研究。  行为主义（Actionism）学派：来源于控制论及“感知—动作”型控制系统。该学派认为智能取决">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ran.ranruo.xyz/img/head_protrait.jpg">
<meta property="article:published_time" content="2024-06-14T12:17:54.000Z">
<meta property="article:modified_time" content="2025-04-11T02:30:57.249Z">
<meta property="article:author" content="白">
<meta property="article:tag" content="课程笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ran.ranruo.xyz/img/head_protrait.jpg"><link rel="shortcut icon" href="/img/favicon1.jpg"><link rel="canonical" href="https://ran.ranruo.xyz/post/3254563453.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '人工智能',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-04-11 02:30:57'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/my_css.css"><meta name="baidu-site-verification" content="codeva-fpDe68QFkn" /><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head_protrait.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background3.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="白のBlog"><span class="site-name">白のBlog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">人工智能</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-14T12:17:54.000Z" title="发表于 2024-06-14 12:17:54">2024-06-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-11T02:30:57.249Z" title="更新于 2025-04-11 02:30:57">2025-04-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%88%91%E9%87%8D%E7%94%9F%E4%BA%86%EF%BC%8C%E9%87%8D%E7%94%9F%E5%9C%A8%E8%80%83%E8%AF%95%E5%89%8D%E4%B8%80%E5%A4%A9/">我重生了，重生在考试前一天</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="人工智能"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="人工智能的三大学派"><a href="#人工智能的三大学派" class="headerlink" title="人工智能的三大学派"></a>人工智能的三大学派</h1><p>人工智能主要学派分为符号主义、连接主义、行为主义。</p>
<ul>
<li><p>符号主义（Symbolicism）学派：认为人工智能源于数理逻辑。该学派将数学严格公理化，从公理出发，由逻辑推理得到引理，定理，推论。</p>
</li>
<li><p>连接主义（Connectionism）学派：认为人工智能源于仿生学，特别是对人脑模型的研究。</p>
</li>
<li><p>行为主义（Actionism）学派：来源于控制论及“感知—动作”型控制系统。该学派认为智能取决于感知和行动，人工智能可以像人类智能一样逐步进化，以及智能行为只能在现实世界中与周围环境交互作用而表现出来。</p>
<script type="math/tex; mode=display">
\begin{array}{|c|c|c|}
\hline \text { 学习模式 } & \text { 优势 } & \text { 不足 } \\
\hline \text { 用规则教 } & \begin{array}{c}
\text { 与人类逻辑推理相似, 解 } \\
\text { 释性强 }
\end{array} & \begin{array}{c}
\text { 难以构建完备的知识规则 } \\
\text { 库 }
\end{array} \\
\hline \text { 用数据学 } & \text { 直接从数据中学 } & \begin{array}{c}
\text { 以深度学习为例: 依赖于 } \\
\text { 数据、解释性不强 }
\end{array} \\
\hline \text { 用问题引导 } & \begin{array}{c}
\text { 从经验中进行能力的持续 } \\
\text { 学习 }
\end{array} & \begin{array}{c}
\text { 非穷举式搜索而需更好策 } \\
\text { 略 }
\end{array} \\
\hline
\end{array}</script></li>
</ul>
<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><h2 id="智能之根：数据"><a href="#智能之根：数据" class="headerlink" title="智能之根：数据"></a>智能之根：数据</h2><ul>
<li>人工智能的本质是从数据中挖掘知识，实现机器智能化的过程。</li>
<li>样例学习是智能体的基本学习方式之一。</li>
</ul>
<h2 id="数据的认识"><a href="#数据的认识" class="headerlink" title="数据的认识"></a>数据的认识</h2><ul>
<li>数据概念：数据对象及其属性的集合。</li>
<li>属性（变量、特征）：对象的属性或特性。</li>
<li>属性值：属性分配的数字或符号。<strong>属性值不一定反映出属性</strong></li>
<li>数据类型：类别属性、顺序属性、区间属性、比率属性。</li>
</ul>
<h2 id="数据集的类型"><a href="#数据集的类型" class="headerlink" title="数据集的类型"></a>数据集的类型</h2><ul>
<li>记录型数据（Record Data）：包含固定属性集的记录集合。</li>
<li>图/网络数据（Graph Data）：如社交网络、分子结构等。</li>
<li>有序数据（Ordered Data）：序列数据、时间序列数据、时空数据等。</li>
</ul>
<h2 id="数据质量"><a href="#数据质量" class="headerlink" title="数据质量"></a>数据质量</h2><p>数据的质量决定了算法的上限</p>
<ul>
<li>数据质量问题：噪声、异常值、缺失值、重复数据。</li>
<li>数据预处理：聚合、采样、降维、离散化、属性转换、特征创建、特征子集选择。</li>
</ul>
<h2 id="数据预处理技术"><a href="#数据预处理技术" class="headerlink" title="数据预处理技术"></a>数据预处理技术</h2><ul>
<li>聚合（Aggregation）：将多个属性或对象合并为单一属性或对象。</li>
<li>采样（Sampling）：数据选择的主要技术，用于初步调研和最终分析。</li>
<li><p>降维（Dimensionality Reduction）：减少数据集的维度，避免“维度的诅咒”。</p>
<ul>
<li>数据的维度越高，数据分散的就越分散。很多机器学习的方法都是基于距离的，在高维的表现不好，所以要将维度下降下来。常用的有PCA，T-SNE，SVD。</li>
</ul>
</li>
</ul>
<blockquote>
<p>T-SNE（t-Distributed Stochastic Neighbor Embedding）是一种非线性降维技术，通常用于高维数据的可视化。它是由Laurens van der Maaten和Geoffrey Hinton在2008年提出的。T-SNE的主要目的是将高维空间中的数据点映射到低维空间（通常是2维或3维），同时保持原始数据中的相似性结构。<br>T-SNE的特点包括但不限于：</p>
<ol>
<li><strong>非线性降维</strong>：与传统的线性降维技术（如PCA）不同，T-SNE能够捕捉数据中的非线性结构。</li>
<li><strong>相似性保持</strong>：T-SNE尝试在低维空间中保持高维空间中数据点之间的相似性。这意味着在高维空间中相似的数据点在低维空间中仍然相似，而不相似的数据点在低维空间中则被拉开。</li>
</ol>
<ul>
<li>离散化（Discretization）：将连续属性转换为分类属性。</li>
<li>属性转换（Attribute Transformation）：对属性值集映射到新的替代值集。</li>
<li>特征创建（Feature Creation）：从原始数据创建新特征，以更有效地捕捉重要信息。</li>
<li>特征子集选择（Feature Subset Selection）：减少数据维度，剔除冗余和不相关特征。</li>
</ul>
</blockquote>
<h2 id="从数据到智能的不同理解方式"><a href="#从数据到智能的不同理解方式" class="headerlink" title="从数据到智能的不同理解方式"></a>从数据到智能的不同理解方式</h2><ul>
<li>符号主义（Symbolism）：使用符号系统和规则来理解数据。</li>
<li>连接主义（Connectionism）：使用网络和学习算法来理解数据。</li>
<li>行为主义（Behaviorism）：使用环境和反馈来理解数据。</li>
</ul>
<h1 id="知识的表示与推理"><a href="#知识的表示与推理" class="headerlink" title="知识的表示与推理"></a>知识的表示与推理</h1><p>人工智能的本质是知识的表示</p>
<ul>
<li>人工智能的成功与否取决于其知识表示的清晰度。</li>
</ul>
<p>知识与数据、信息的区别：</p>
<ul>
<li>数据：单独的事实、信号、符号，是信息的载体。</li>
<li>信息：由符号组成，如文字和数字，赋予符号一定的意义。</li>
<li>知识：在信息的基础上增加上下文信息，提供更多意义。</li>
</ul>
<p>知识表示：将知识符号化并输入到计算机的过程和方法。</p>
<p>知识表示的两种基本观点：</p>
<ul>
<li>陈述性知识表示：知识表示与应用分开处理。</li>
<li>过程性知识表示：知识表示与应用结合，知识与程序融合。</li>
</ul>
<h2 id="现代逻辑学"><a href="#现代逻辑学" class="headerlink" title="现代逻辑学"></a>现代逻辑学</h2><h3 id="命题逻辑"><a href="#命题逻辑" class="headerlink" title="命题逻辑"></a>命题逻辑</h3><p><strong>命题</strong>：具有真假意义的陈述句。</p>
<p><strong>命题逻辑的组成</strong>：</p>
<ul>
<li>原子命题：最基本的命题。</li>
<li>复合命题：由联结词、标点符号和原子命题复合构成。</li>
</ul>
<blockquote>
<p>具体的公式参考离散数学以及人工智能课件</p>
</blockquote>
<h2 id="归结推理"><a href="#归结推理" class="headerlink" title="归结推理"></a>归结推理</h2><h3 id="1-子句"><a href="#1-子句" class="headerlink" title="1. 子句"></a>1. 子句</h3><p>在归结推理中，<strong>子句</strong>是一个关键概念。一个子句是逻辑公式的一个特殊形式，可以表示为：</p>
<script type="math/tex; mode=display">
L_1 \vee L_2 \vee \ldots \vee L_n</script><p>其中，每个 $ L_i $ 都是一个文字（literal），文字可以是原子公式或其否定。子句也可以是空的，表示为 $\square$ 或 $\bot$，这在逻辑中代表矛盾或“假”。</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>子句可以包含任意数量的文字。</li>
<li>子句中的文字可以是正的（原子公式）或负的（原子公式的否定）。</li>
<li>空子句表示逻辑矛盾。<h3 id="2-谓词公式转子句集"><a href="#2-谓词公式转子句集" class="headerlink" title="2. 谓词公式转子句集"></a>2. 谓词公式转子句集</h3></li>
</ul>
<p>要将下列谓词公式化为子句集：</p>
<script type="math/tex; mode=display">\forall x\{[\neg P(x) \vee \neg Q(x)] \rightarrow \exists y[S(x, y) \wedge Q(x)]\} \wedge \forall x[P(x) \vee B(x)]</script><ol>
<li><p><strong>消去蕴涵符号</strong><br>将原公式中的蕴涵符号（$\rightarrow$）转换为或（$\vee$）与非（$\wedge$）的组合：</p>
<script type="math/tex; mode=display">
\forall x\{\neg[\neg P(x) \vee \neg Q(x)] \vee \exists y[S(x, y) \wedge Q(x)]\} \wedge \forall x[P(x) \vee B(x)]</script></li>
<li><p><strong>移动否定符号</strong><br>将否定符号（$\neg$）移到每个谓词前：</p>
<script type="math/tex; mode=display">
\forall x\{[P(x) \wedge Q(x)] \vee \exists y[S(x, y) \wedge Q(x)]\} \wedge \forall x[P(x) \vee B(x)]</script></li>
<li><p><strong>变量标准化</strong><br>为了避免变量冲突，对变量进行标准化处理：</p>
<script type="math/tex; mode=display">
\forall x\{[P(x) \wedge Q(x)] \vee \exists y[S(x, y) \wedge Q(x)]\} \wedge \forall w[P(w) \vee B(w)]</script></li>
<li><p><strong>消去存在量词</strong><br>设 $y$ 的Skolem函数是 $f(x)$，则：</p>
<script type="math/tex; mode=display">
\forall x\{[P(x) \wedge Q(x)] \vee[S(x, f(x)) \wedge Q(x)]\} \wedge \forall w[P(w) \vee B(w)]</script></li>
<li><p><strong>化为前束型</strong><br>将公式化为前束型，即将量词移至公式最前面：</p>
<script type="math/tex; mode=display">
\forall x \forall w\{\{[P(x) \wedge Q(x)] \vee[S(x, f(x)) \wedge Q(x)]\} \wedge[P(w) \vee B(w)]\}</script></li>
<li><p><strong>化为标准形</strong><br>将公式化为标准形，包括整理合取与析取：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\forall x \forall w\{\{[Q(x) \wedge P(x)] \vee[Q(x) \wedge S(x, f(x))]\} \wedge[P(w) \vee B(w)]\} \\
\forall x \forall w\{Q(x) \wedge[P(x) \vee S(x, f(x))] \wedge[P(w) \vee B(w)]\}
\end{array}</script></li>
<li><p><strong>略去全称量词</strong><br>在这一步中，我们去掉全称量词：</p>
<script type="math/tex; mode=display">
Q(x) \wedge[P(x) \vee S(x, f(x))] \wedge[P(w) \vee B(w)]</script></li>
<li><p><strong>消去合取词</strong><br>将公式分解为子句集，每个合取词对应一个子句：</p>
<script type="math/tex; mode=display">
\{Q(x),(P(x), S(x, f(x))),(P(w), B(w))\}</script></li>
<li><p><strong>子句变量标准化</strong><br>对子句中的变量进行标准化处理：</p>
<script type="math/tex; mode=display">
\{Q(x),(P(y), S(y, f(y))),(P(w), B(w))\}</script></li>
</ol>
<h3 id="3-归结式"><a href="#3-归结式" class="headerlink" title="3. 归结式"></a>3. 归结式</h3><p><strong>归结式</strong>（Resolution Inference Rule）是归结推理的核心。它是一个推导规则，用于从两个子句推导出一个新的子句。归结式的应用基于以下原则：</p>
<ul>
<li>选择两个子句，它们至少有一个共同的文字，且在一个子句中为正，在另一个中为反。</li>
<li>删除这两个子句中的共同文字及其否定。</li>
<li>合并剩余的文字形成一个新的子句。<h4 id="归结式的表示"><a href="#归结式的表示" class="headerlink" title="归结式的表示"></a>归结式的表示</h4>如果 $ C_1 $ 和 $ C_2 $ 是两个子句，并且它们有一个共同的文字 $ L $ 和它的否定 $ \neg L $，则归结式可以表示为：<script type="math/tex; mode=display">
C_1, C_2 \rightarrow C_1 \cup C_2 - \{L, \neg L\}</script>其中 $ C_1 \cup C_2 $ 表示两个子句文字的并集，$ -\{L, \neg L\} $ 表示去除共同的文字和它的否定。</li>
</ul>
<p>假设有两个子句：</p>
<ul>
<li>$ C_1: A \vee B \vee \neg C $</li>
<li>$ C_2: \neg A \vee D $<br>它们可以通过归结式来推导出一个新的子句：</li>
<li>共同文字：$ A $ 和 $ \neg A $</li>
<li>应用归结式：删除 $ A $ 和 $ \neg A $，得到新的子句 $ B \vee \neg C \vee D $</li>
</ul>
<h3 id="4-归结推导"><a href="#4-归结推导" class="headerlink" title="4. 归结推导"></a>4. 归结推导</h3><h4 id="命题逻辑-1"><a href="#命题逻辑-1" class="headerlink" title="命题逻辑"></a>命题逻辑</h4><ol>
<li>前提转换：将所有前提转换为子句的合取范式。</li>
<li>归结：应用归结规则，选择两个子句进行归结，生成新的子句。</li>
<li>重复归结：使用新生成的子句重复归结步骤。</li>
<li>检查结果：检查是否生成了空子句。如果生成了空子句，则推导成功（说明之前的亲本子句也是假的，一直向前传递）；如果没有生成空子句且无法继续归结，则推导失败。</li>
</ol>
<h4 id="谓词逻辑"><a href="#谓词逻辑" class="headerlink" title="谓词逻辑"></a>谓词逻辑</h4><p><strong>谓词逻辑在归结的时候还可能需要进行置换和合一</strong>。</p>
<h5 id="置换和合一"><a href="#置换和合一" class="headerlink" title="置换和合一"></a>置换和合一</h5><p>置换：可以用$t_1 /v_1$或$v_1=t_1$来表示用$t_1$置换$v_1$​</p>
<ul>
<li>$t_i$不允许包含与$v_i$有关的项</li>
<li>不允许$v_i$循环出现在另一个$t_i$​中作置换</li>
</ul>
<p>合一：存在置换$\lambda$使得$\mathrm{F}_{1} \lambda=\mathrm{F}_{2} \lambda$，则$\lambda$称为一个合一。（不唯一）</p>
<p>最一般合一：定义设  $\sigma$  是公式集$F$的一个合一，如果对任一个合一  $\theta$  都存在一个置换  $\lambda$ , 使得  $\theta=\sigma\circ \lambda$ 则称  $\sigma$​​  是一个最一般合一（MGU）。（唯一）</p>
<h5 id="最一般合一算法"><a href="#最一般合一算法" class="headerlink" title="最一般合一算法"></a>最一般合一算法</h5><ol>
<li><p>令 $k=0$, $F_k=F$, $\sigma_k=\varepsilon$。其中 $\varepsilon$ 是空置换。</p>
</li>
<li><p>若 $F_k$ 只含一个表达式，则算法停止，$\sigma_k$ 就是最一般合一。</p>
</li>
<li><p>找出 $F_k$ 的差异集 $D_k$。</p>
</li>
<li><p>若 $D_k$ 中存在元素 $x_k$ 和 $t_k$，其中 $x_k$ 是变元，$t_k$ 是项，且 $x_k$ 不在 $t_k$ 中出现，则置：</p>
<script type="math/tex; mode=display">
\begin{align*}
F_{k+1} & = F_k \{ t_k / x_k \} \\
\sigma_{k+1} & = \sigma_k \{ t_k / x_k \} \\
k & = k + 1
\end{align*}</script><p>然后转 (2)。若不存在这样的 $x_k$ 和 $t_k$ 则算法停止。</p>
</li>
<li><p>算法终止，$F$​的最一般合一不存在。</p>
</li>
</ol>
<h3 id="5-归结实例"><a href="#5-归结实例" class="headerlink" title="5. 归结实例"></a>5. 归结实例</h3><p>设A、B、C三人中有人从不说真话，也有人从不说假话。某人向这三人分别提出同一个问题：谁是说谎者？以下是他们的回答：</p>
<ul>
<li>A答：“B和C都是说谎者”；</li>
<li>B答：“A和C都是说谎者”；</li>
<li>C答：“A和B中至少有一个是说谎者”。</li>
</ul>
<p>求谁是老实人，谁是说谎者？</p>
<p>先写出公式：</p>
<script type="math/tex; mode=display">
\begin{array}{l} 
\mathrm{T}(\mathrm{C}) \vee \mathrm{T}(\mathrm{A}) \vee \mathrm{T}(\mathrm{B}) \\
\neg \mathrm{T}(\mathrm{C}) \vee \neg \mathrm{T}(\mathrm{A}) \vee \neg \mathrm{T}(\mathrm{B}) \\
\mathrm{T}(\mathrm{A}) \rightarrow \neg \mathrm{T}(\mathrm{B}) \wedge \neg \mathrm{T}(\mathrm{C}) \\
\neg \mathrm{T}(\mathrm{A}) \rightarrow \mathrm{T}(\mathrm{B}) \vee \mathrm{T}(\mathrm{C}) \\
\mathrm{T}(\mathrm{B}) \rightarrow \neg \mathrm{T}(\mathrm{A}) \wedge \neg \mathrm{T}(\mathrm{C}) \\
\neg \mathrm{T}(\mathrm{B}) \rightarrow \mathrm{T}(\mathrm{A}) \vee \mathrm{T}(\mathrm{C}) \\
\mathrm{T}(\mathrm{C}) \rightarrow \neg \mathrm{T}(\mathrm{A}) \vee \neg \mathrm{T}(\mathrm{B}) \\
\neg \mathrm{T}(\mathrm{C}) \rightarrow \mathrm{T}(\mathrm{A}) \wedge \mathrm{T}(\mathrm{B})
\end{array}</script><p>然后，我们把上述回答公式化成子句集S：</p>
<ol>
<li>¬T(A) ∨ ¬T(B) （如果A不是老实人，则B不是老实人）</li>
<li>¬T(A) ∨ ¬T(C) （如果A不是老实人，则C不是老实人）</li>
<li>T(C) ∨ T(A) ∨ T(B) （C、A、B中至少有一个是老实人）</li>
<li>¬T(B) ∨ ¬T(C) （如果B不是老实人，则C不是老实人）</li>
<li>¬T(C) ∨ ¬T(A) ∨ ¬T(B) （C、A、B中至少有一个不是老实人）</li>
<li>T(A) ∨ T(C) （A或C是老实人）</li>
<li>T(B) ∨ T(C) （B或C是老实人）</li>
</ol>
<p>下面先求谁是老实人。把¬T(x) ∨ Answer(x)并入$S$得到$S_1$。即多一个子句：</p>
<ol>
<li>¬T(x) ∨ Answer(x) （如果x不是老实人，则x是答案）</li>
</ol>
<p>应用归结原理对$S_1$进行归结：</p>
<ol>
<li>¬T(A) ∨ T(C) （子句1和子句7归结）</li>
<li>T(C) （子句6和子句9归结）</li>
<li>Answer(C) （子句8和子句10归结）</li>
</ol>
<p>所以C是老实人，即C从不说假话。</p>
<h3 id="6-归结策略"><a href="#6-归结策略" class="headerlink" title="6. 归结策略"></a>6. 归结策略</h3><p>宽度优先、支持集策略、线性输入策略（不完备）、单文字策略（不完备）、祖先过滤策略。</p>
<p>删除策略：纯文字删除法、重言式删除法、包孕删除法。</p>
<h2 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h2><p>知识图谱是由一些相互连接的实体及其属性构成的</p>
<p>三元组是知识图谱的一种通用表示方式：</p>
<ul>
<li>(实体1-关系-实体2)：中国-首都-北京</li>
<li>(实体-属性-属性值)：北京-人口-2069万</li>
</ul>
<p><strong>知识库推理包括基于符号的推理和基于统计的推理。</strong></p>
<h3 id="知识图谱推理：FOIL算法"><a href="#知识图谱推理：FOIL算法" class="headerlink" title="知识图谱推理：FOIL算法"></a>知识图谱推理：FOIL算法</h3><ol>
<li><p><strong>初始化</strong>：选择一个目标谓词（例如，<code>Father(X, Y)</code>），这是我们想要学习推理规则的谓词。</p>
</li>
<li><p><strong>前提约束谓词的选择</strong>：从知识图谱中选择可能作为目标谓词前提约束的谓词（例如，<code>Mother(X, Y)</code>, <code>Sibling(X, Y)</code> 等）。</p>
</li>
<li><p><strong>正例和反例的构造</strong>：根据知识图谱中的信息，构造出目标谓词的正例和反例。正例是已知满足目标谓词的实体对，而反例是不满足目标谓词的实体对。</p>
</li>
<li><p><strong>信息增益计算</strong>：对于每个候选的前提约束谓词，计算其加入推理规则后的信息增益值。</p>
<script type="math/tex; mode=display">
\text {增益值 }=\widehat{m_{+}} \cdot\left(\log _{2} \frac{\widehat{m_{+}}}{\widehat{m_{+}}+\widehat{m_{-}}}-\log _{2} \frac{m_{+}}{m_{+}+m_{-}}\right)</script></li>
<li><p><strong>选择最佳前提</strong>：基于信息增益值，选择最佳的前提约束谓词加入到推理规则中。</p>
</li>
<li><p><strong>推理规则的构建</strong>：将选定的前提约束谓词与目标谓词结合，形成新的推理规则（例如，<code>Couple(X, Z) -&gt; Father(X, Y)</code>）。将训练样例中与该推理规则不符的样例（正例和反例）去掉。</p>
</li>
<li><p><strong>规则评估</strong>：评估新构建的推理规则是否满足所有正例并且不覆盖任何反例。如果满足，规则学习完成，得到最终的推理规则；如果不满足，返回步骤5，选择另一个前提约束谓词并重复过程。</p>
</li>
</ol>
<h1 id="搜索技术"><a href="#搜索技术" class="headerlink" title="搜索技术"></a>搜索技术</h1><h2 id="盲目搜索"><a href="#盲目搜索" class="headerlink" title="盲目搜索"></a>盲目搜索</h2><p>盲目搜索有三个特点：</p>
<ul>
<li>这些策略都采用固定的规则来选择下一需要被扩展的状态</li>
<li>这些规则不会随着要搜索解决的问题的变化而变化</li>
<li>这些策略不考虑任何与要解决的问题领域相关的信息</li>
</ul>
<p>搜索算法的重要特征：</p>
<ul>
<li>完备性（Completeness）: 搜索算法是否总能在问题存在解的情况下找到解</li>
<li>最优性（Optimality）: 当问题中的动作是需要成本时，搜索算法是否总能找到成本最小的解</li>
<li>时间复杂度（Time complexity）: 搜索算法最多需要探索/生成多少个节点来找到解</li>
<li>空间复杂度（Space complexity）: 搜索算法最多需要将多少个节点储存在内存中</li>
</ul>
<h3 id="常用的方法"><a href="#常用的方法" class="headerlink" title="常用的方法"></a>常用的方法</h3><ul>
<li>深度优先：把当前要扩展的状态的后继状态放在边界的最前面<ul>
<li>时间复杂度为$O(b^m),m是遍历过程中最长路径的长度$​</li>
<li>空间复杂度为$O(bm)$，线性复杂度</li>
<li>不具有完备性和最优性</li>
</ul>
</li>
<li>宽度优先：把当前要扩展的状态的后继状态放在边界的最后<ul>
<li>时间复杂度为$O(b^{d+1}),d=最短解的个数$</li>
<li>空间复杂度高</li>
<li>具有完备性和最优性</li>
</ul>
</li>
<li>一致代价：边界中，按路径的成本升序排列；总是扩展成本最低的那条路径<ul>
<li>只是将宽度优先算法的队列换成了优先队列</li>
<li>如果最优解为$C<em>$，则时间和空间复杂度为 $O\left(b^{C </em> / s+1}\right)$</li>
</ul>
</li>
<li>深度受限：深度优先搜索，但是预先限制了搜索的深度 $L$​<ul>
<li>不具有完备性和最优性</li>
<li>时间复杂度$O(b^L)$，空间复杂度$O(bL)$</li>
</ul>
</li>
<li><p>迭代加深搜索：一开始设置深度限制为$L = 0$，迭代地增加深度限制，对于每个深度限制都进行深度受限搜索</p>
<ul>
<li>具有完备性和最优性</li>
<li>时间复杂度为$O(b^{d}),d=最短解的个数$​</li>
<li>空间复杂度$O(bd)$</li>
</ul>
</li>
<li><p>双向搜索：同时进行从初始状态向前的搜索和从目标节点向后搜索，在</p>
<p>两个搜索在中间相遇时停止搜索</p>
<ul>
<li>难点：如何向后搜索</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">标准</th>
<th style="text-align:left">广度优先搜索</th>
<th>统一成本搜索</th>
<th>深度优先搜索</th>
<th>深度受限搜索</th>
<th>迭代深化搜索</th>
<th>双向搜索（如果适用）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">是否完整？</td>
<td style="text-align:left">是 (a)</td>
<td>是 (a, b)</td>
<td>否</td>
<td>否</td>
<td>是 (a)</td>
<td>是 (a, d)</td>
</tr>
<tr>
<td style="text-align:center">时间复杂度</td>
<td style="text-align:left">$O(b^d)$</td>
<td>$O(b^{1+\lfloor C^*/\varepsilon \rfloor})$</td>
<td>$O(b^m)$</td>
<td>$O(b^\ell)$</td>
<td>$O(b^d)$</td>
<td>$O(b^{d/2})$</td>
</tr>
<tr>
<td style="text-align:center">空间复杂度</td>
<td style="text-align:left">$O(b^d)$</td>
<td>$O(b^{1+\lfloor C^*/\varepsilon \rfloor})$</td>
<td>$O(bm)$</td>
<td>$O(b\ell)$</td>
<td>$O(bd)$</td>
<td>$O(b^{d/2})$</td>
</tr>
<tr>
<td style="text-align:center">是否最优？</td>
<td style="text-align:left">是 (c)</td>
<td>是</td>
<td>否</td>
<td>否</td>
<td>是 (c)</td>
<td>是 (c, d)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="路径检测"><a href="#路径检测" class="headerlink" title="路径检测"></a>路径检测</h3><p>路径检测：当扩展节点$n$来获得子节点$c$时，确保节点$c$不等于到达节点$c$的路径上的任何祖先节点。</p>
<p>环检测：记录下在之前的搜索过程中扩展过的所有节点，当扩展节点 $n_k$ 获得子节点$c$​时，确保节点c不等于<strong>之前任何扩展过的节点</strong></p>
<ul>
<li><p>对于一致代价搜索，使用环检测后仍能找到最优的解</p>
</li>
<li><p>对于启发式搜索，这个性质不一定会成立</p>
</li>
</ul>
<h2 id="启发搜索"><a href="#启发搜索" class="headerlink" title="启发搜索"></a>启发搜索</h2><p>常见的启发函数$h(n)$：欧氏距离、曼哈顿距离</p>
<p>欧氏距离：$\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + … + (x_n - y_n)^2}$​</p>
<p>曼哈顿距离：$| x_1 - y_1 | + | x_2 - y_2 | + … + | x_n - y_n |$</p>
<p>贪心最佳优先搜索：只考虑启发式函数$h(n)$​来对边界上的节点进行排序</p>
<ul>
<li>但是，这种做法忽略了从初始状态到达节点𝑛的成本</li>
<li>其是不完备的</li>
</ul>
<h3 id="A搜索"><a href="#A搜索" class="headerlink" title="A搜索"></a>A搜索</h3><p>盲目搜索只考虑了前半部分，能计算出从初始节点走到当前节点的优劣。启发函数则只“估计”了当前节点到最终节点的优劣。</p>
<p>两者相结合，就是启发式搜索策略，典型的代表是A算法。</p>
<p>其定义了评价函数$𝑓 (𝑛)$，利用节点对应的$𝑓 (𝑛)$值来对边界上的节点进行排序，并总扩展边界中具有最小$𝑓$​ 值的节点。$g(n)$是到$n$点的实际开销，$h(n)$是$n$点到终点的估计距离。</p>
<script type="math/tex; mode=display">
f(n)=g(n)+h(n)</script><p>算法步骤：</p>
<ol>
<li>将初始节点的加入优先队列中。</li>
<li>取优先队列中$f(n)$最小的结点。</li>
<li>如果这个结点不是目标结点，则计算其相邻的结点的$f(n)$值，并将这些节点加入优先队列中。转2。</li>
<li>算法终止。</li>
</ol>
<h4 id="启发函数的要求"><a href="#启发函数的要求" class="headerlink" title="启发函数的要求"></a>启发函数的要求</h4><ul>
<li>如果启发函数的值太大，超出了实际的cost，就无法找到最优解</li>
<li>如果启发函数的值太小，A搜索就变为了一致代价搜索</li>
</ul>
<h3 id="A-搜索"><a href="#A-搜索" class="headerlink" title="A*搜索"></a>A*搜索</h3><p>在A算法中，如果代价函数  $f(n)=g(n)+h(n)$  始终满足  $h(n) \leqq h^{<em>}(n)$  那么该算法就是  $\mathbf{A}^{</em>}$​  算法。</p>
<p>一个好的$h(n)$​要满足的条件有两个：</p>
<blockquote>
<p>$ h(n)=0  时，对于任何  \mathrm{n}  这个启发式函数都是单调的。  \mathrm{A} *  搜索会退化成一致代价搜索$</p>
</blockquote>
<ul>
<li>可采纳性：</li>
</ul>
<script type="math/tex; mode=display">
\text { 当对于所有节点 } n \text { ，满足 } h(n) \leq h^{*}(n) ， h(n) \text { 是可采纳的 }</script><p>可采纳的启发式函数<strong>低估了当前节点到达目标节点的成本</strong>，使得实际成本最小的最优路径能够被选上。</p>
<p><mark>可采纳性意味着最优性</mark>：$\text { 最优解一定会在所有成本大于 } C^{*} \text { 的路径之前被扩展到 }$</p>
<ul>
<li><p>单调性（一致性）：</p>
<script type="math/tex; mode=display">
对于任意节点  n_{1}  和  n_{2} , 若，h\left(n_{1}\right) \leq c\left(n_{1} \rightarrow n_{2}\right)+h\left(n_{2}\right)，则  h(n)  具有单调性</script><p>如果是大于号，代表过大估计了cost。</p>
<p>如果具有单调性，则具有下面以下性质：</p>
<ol>
<li><p><mark><strong>满足一致性的启发式函数也一定满足可采纳性</strong></mark></p>
</li>
<li><p><mark><strong>在进行环检测之后仍然保持最优性</strong></mark></p>
</li>
<li>一条路径上的节点的 $f$​ 函数值应该是非递减的</li>
<li>如果节点  $n_2$  在节点  $n_1$  之后被扩展, 则有$f(n_1) \leq f(n_2)$</li>
<li>在遍历节点$n$时，所有$𝑓$值小于$𝑓(𝑛)$​​的节点都已经被遍历过了</li>
<li>A*搜索第一次扩展到某个状态，其已经找到到达该状态的最小成本路径</li>
</ol>
<blockquote>
<p>拓展指的是将该节点相邻的结点加入边界，遍历指的是探索这个结点</p>
</blockquote>
</li>
</ul>
<p>如果$h(n)$只满足可采纳性但是不满足单调性：</p>
<ol>
<li>虽然确实可以找到最优路径，但是搜索过程可能会更久。</li>
<li><strong>如果启发式函数只有可采纳性，则不一定能在使用了环检测之后仍保持最优性</strong>。若出现到达已遍历过节点但成本更低的路径，则需重新扩展而不能剪枝。</li>
</ol>
<h3 id="IDA-算法"><a href="#IDA-算法" class="headerlink" title="IDA*算法"></a>IDA*算法</h3><p>A∗搜索 和宽度优先搜索（BFS）或一致代价搜索（UCS）一样存在潜在的空间复杂度过大的问题。 IDA∗ （迭代加深的A∗搜索）与迭代加深搜索一样用于解决空间复杂度的问题。</p>
<p>但用于划定界限的不是深度，而是使用 $f$ 值$\left(g+h\right)$​</p>
<p>当启发函数h为可采纳时， IDA* 是最优的。</p>
<h3 id="松弛问题"><a href="#松弛问题" class="headerlink" title="松弛问题"></a>松弛问题</h3><p>可以通过放宽原始问题中的一些约束条件来简化问题，从而使得问题更容易解决。</p>
<blockquote>
<p>例如，在8数码问题中，原始问题要求将一些乱序的数字块通过滑动操作（只能移动到相邻的空格）恢复到正确的顺序。为了构建启发式函数，我们可以设计一个松弛问题，其中允许我们在任何时候都将一个数字块移动到目标位置，而不需要考虑是否相邻或空位的限制。这样的松弛问题简化了原始问题，因为它忽略了移动的约束条件，从而更容易计算出解决方案的成本。</p>
</blockquote>
<p>在松弛问题中，到达某个节点的最优成本可作为原始问题中到达该节点的可采纳的启发式函数值。</p>
<h2 id="对抗性搜索"><a href="#对抗性搜索" class="headerlink" title="对抗性搜索"></a>对抗性搜索</h2><p>上述的两个搜索算法都假设智能体对环境有完全的控制。</p>
<p>对抗搜索（博弈）的主要特点：超过两个以上玩家且均可以改变状态。难点在于对方会如何行动。博弈有确定的和随机的、完全的信息和不完全的信息等不同的特征.</p>
<p>经典的博弈问题就是双人博弈的问题，主要关注扩展形式的博弈：</p>
<ul>
<li>两个玩家：游戏的状态或决策可以映射为离散的值，游戏的状态或可以采取的行动的种类是有限的。</li>
<li>零和博弈：游戏的一方赢了，则另一方输掉了同等的数量</li>
<li>确定性：没有不确定的因素</li>
<li>完整的信息：任何层面的状态都是可观察的</li>
</ul>
<h3 id="MinMax策略"><a href="#MinMax策略" class="headerlink" title="MinMax策略"></a>MinMax策略</h3><p>对这种扩展式的双人博弈，采用MINMAX搜索。搜索的步骤如下：</p>
<ul>
<li>构建完整的博弈树（每个叶子节点都表示终止状态）</li>
<li>反向传播效益值$U(n)$：<ul>
<li>叶子节点的$U(n)$是提前定义的</li>
<li>如果结点$n$是一个$Min$节点：$U(n)=min\{U(c):c是n的子节点\}$</li>
<li>如果结点$n$是一个$Max$节点：$U(n)=Max\{U(c):c是n的子节点\}$</li>
</ul>
</li>
</ul>
<p>为了解决博弈树太大的问题，需要使用深度优先搜索算法来实现MinMax。</p>
<h3 id="Alpha-Beta剪枝"><a href="#Alpha-Beta剪枝" class="headerlink" title="Alpha-Beta剪枝"></a>Alpha-Beta剪枝</h3><p>为了进一步提高MinMax执行效率，可以对没有必要探索的分支进行剪枝。</p>
<p>Alpha-Beta剪枝的算法如下：（考试要求写的Alpha-Beta算法）</p>
<ol>
<li>初始化根节点的$\alpha$值为$-\infty$，$\beta$值为$+\infty$​</li>
<li>以后序遍历遍历整棵博弈树。<ol>
<li>每一个子节点都继承父节点的$\alpha$值和$\beta$值</li>
<li>每当一个子节点的被遍历完或$\alpha \geq \beta$时，停止遍历其并将结点值返回父节点并更新其父节点的结点值和$\alpha$值或$\beta$值。</li>
<li>MAX结点更新$\alpha$值，MIN结点更新$\beta$值</li>
</ol>
</li>
</ol>
<p>具体而言，如果一个节点是MIN结点，其的$\alpha$值是继承父节点MAX结点的，$\beta$值是自己更新的。当$\alpha \geq \beta$的时候，因为MIN结点的取值只会越来越小，而父节点（MAX结点）能取到的值已经大于该子节点的值了，所以已经不需要继续遍历了，可以直接返回。</p>
<p>如果一个节点是MAX结点，其的$\beta$值是继承父节点MIN节点的,$\alpha$值是自己更新的。当$\alpha \geq \beta$​的时候，因为MAX结点的取值只会越来越大，而父节点（MIN结点）能取到的值已经小于该子节点的值了，所以已经不需要继续遍历了，可以直接返回。</p>
<p>算法的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SearchGameTree</span>(<span class="params">alpha,beta,root,level</span>):</span><br><span class="line">  <span class="comment">#叶节点则返回节点的值</span></span><br><span class="line">  <span class="keyword">if</span> IsLeaf(root):</span><br><span class="line">    <span class="keyword">return</span> root.value</span><br><span class="line">  </span><br><span class="line">  value=infinity <span class="keyword">if</span> level==<span class="string">&quot;Min&quot;</span> <span class="keyword">else</span> -infinity</span><br><span class="line">  <span class="comment">#其他结点则返回alpha或beta值</span></span><br><span class="line">  <span class="keyword">for</span> child <span class="keyword">in</span> childsOf(root):</span><br><span class="line">    <span class="comment">#每有一个子节点更新，则更新结点的值，并</span></span><br><span class="line">    <span class="keyword">if</span>(level==<span class="string">&quot;Max&quot;</span>):</span><br><span class="line">    	value=<span class="built_in">max</span>(value,SearchGameTree(alpha,beta,child,<span class="string">&quot;Min&quot;</span>))</span><br><span class="line">      alpha=<span class="built_in">max</span>(alpha,value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	value=<span class="built_in">min</span>(value,SearchGameTree(alpha,beta,child,<span class="string">&quot;Max&quot;</span>))</span><br><span class="line">      beta=<span class="built_in">min</span>(beta,value)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">if</span>(alpha&gt;=beta):</span><br><span class="line">      <span class="keyword">return</span> value</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure>
<blockquote>
<p>一种优化的Alpha-Beta剪枝的算法如下：</p>
<ol>
<li>初始化根节点的$\alpha$值为$-\infty$，$\beta$值为$+\infty$​</li>
<li>以后序遍历遍历整棵博弈树。<ol>
<li>每一个子节点都继承父节点的$\alpha$值和$\beta$值</li>
<li>每当一个节点遍历完或其的$\alpha \geq \beta$，如果其是MAX结点返回$\alpha$值，MIN结点返回$\beta$值</li>
<li>MIN结点只更新$\beta$值，MAX结点只更新$\alpha$​值</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SearchGameTree</span>(<span class="params">alpha,beta,root,level</span>):</span><br><span class="line">  <span class="comment">#叶节点则返回节点的值</span></span><br><span class="line">  <span class="keyword">if</span> IsLeaf(root):</span><br><span class="line">    <span class="keyword">return</span> root.value</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#其他结点则返回alpha或beta值</span></span><br><span class="line">  <span class="keyword">for</span> child <span class="keyword">in</span> childsOf(root):</span><br><span class="line">    <span class="comment">#每有一个子节点更新，则更新结点的alpha-beta值</span></span><br><span class="line">    <span class="keyword">if</span>(level==<span class="string">&quot;Max&quot;</span>):</span><br><span class="line">    	alpha=<span class="built_in">max</span>(alpha,SearchGameTree(alpha,beta,child,<span class="string">&quot;Min&quot;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	beta=<span class="built_in">min</span>(beta,SearchGameTree(alpha,beta,child,<span class="string">&quot;Max&quot;</span>))</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">if</span>(alpha&gt;=beta):</span><br><span class="line">      <span class="keyword">return</span> alpha <span class="keyword">if</span> level==<span class="string">&quot;Max&quot;</span> <span class="keyword">else</span> beta</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">return</span> alpha <span class="keyword">if</span> level==<span class="string">&quot;Max&quot;</span> <span class="keyword">else</span> beta</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="高级搜索"><a href="#高级搜索" class="headerlink" title="高级搜索"></a>高级搜索</h2><p>如果目标路径与问题解不相关，将考虑各种根本不关心路径（耗散）的算法，其中占据重要地位的就是局部搜索算法。</p>
<p>局部搜索算法: 局部搜索算法从单独的一个当前状态出发，通常只移动到与之相邻的状态</p>
<h3 id="爬山算法"><a href="#爬山算法" class="headerlink" title="爬山算法"></a>爬山算法</h3><p>爬山搜索算法是一种局部搜索算法。当它达到一个峰值，没有邻居有更高的值时，它将终止。</p>
<p>伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Hill_Climbing</span>(<span class="params">problem</span>):</span><br><span class="line">  current=problem</span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    neighbor=highest_valued_neighbor(neighbors_of(current));</span><br><span class="line">    <span class="keyword">if</span> neighbor.value&lt;=current.value:</span><br><span class="line">      <span class="keyword">return</span> current</span><br><span class="line">    current=neighbor</span><br></pre></td></tr></table></figure>
<p>爬山算法存在以下问题：</p>
<ul>
<li><strong>局部极大值</strong>：当前状态的邻居状态值都低于当前状态，导致搜索停止。</li>
<li><strong>高原或山肩</strong>：多个局部极大值连成一片，搜索可能无法找到最优解。</li>
</ul>
<p>所以爬山法搜索成功与否在很大程度上取决于状态空间地形图的形状</p>
<h3 id="模拟退火算法"><a href="#模拟退火算法" class="headerlink" title="模拟退火算法"></a>模拟退火算法</h3><p>模拟退火算法（Simulated Annealing, SA）是一种模拟物理退火过程的优化算法，用于解决NP难题和避免陷入局部最优。</p>
<h4 id="基本要素"><a href="#基本要素" class="headerlink" title="基本要素"></a>基本要素</h4><p>三函数两准则一初温：</p>
<ul>
<li>三函数<ul>
<li>状态产生函数</li>
<li>状态接受函数：常使用$min(1,e^{-\frac{\Delta C}{t}})$（会以一定几率接受劣解）</li>
<li>温度更新函数：常使用$t_{k+1}=\alpha t_k$</li>
</ul>
</li>
<li>两准则<ul>
<li>内循环终止准则</li>
<li>外循环终止准则</li>
</ul>
</li>
<li>初始温度</li>
</ul>
<h4 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240407194610480.png" alt="image-20240407194610480"></p>
<blockquote>
<p>计算接受概率$P(t_k)=e^{-[\frac{f(j)-f(i)}{t_k}]}$</p>
</blockquote>
<h3 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h3><h4 id="基本流程-1"><a href="#基本流程-1" class="headerlink" title="基本流程"></a>基本流程</h4><p>遗传算法（Genetic Algorithms, GA）是一类借鉴生物进化机制的随机搜索算法，适用于解决复杂和非线性优化问题。</p>
<p>伪代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">procedure Simple_GA(<span class="built_in">input</span> GA_parameters):</span><br><span class="line">    <span class="comment"># 输入：遗传算法参数</span></span><br><span class="line">    <span class="comment"># 输出：最佳解决方案</span></span><br><span class="line">    generation = <span class="number">0</span>  <span class="comment"># 代数</span></span><br><span class="line">    Population = initialize_population(GA_parameters)  <span class="comment"># 种群</span></span><br><span class="line">    Population.evaluate_fitness()  <span class="comment"># 评估种群的适应度</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 主循环，直到满足终止条件</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> termination_condition(t, GA_parameters):</span><br><span class="line">        Children = crossover(P_t, GA_parameters)  <span class="comment"># Children：交叉后产生的后代</span></span><br><span class="line">        Children = mutate(Children, GA_parameters)  <span class="comment"># 变异Children</span></span><br><span class="line">        Children.evaluate_fitness()  <span class="comment"># 评估Children的适应度</span></span><br><span class="line">        Population = select_next_generation(Population, Children, GA_parameters)  <span class="comment"># 选择下一代的种群</span></span><br><span class="line">        generation = generation + <span class="number">1</span>  <span class="comment"># 增加代数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出最佳解决方案</span></span><br><span class="line">    best_solution = get_best_solution(Population)</span><br><span class="line">    <span class="keyword">return</span> best_solution</span><br></pre></td></tr></table></figure>
<h4 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h4><p>为了交叉（<code>crossover</code>）和变异（<code>mutate</code>），需要对问题状态进行编码，常用编码方式有：</p>
<ul>
<li>二进制编码</li>
<li>格雷编码</li>
<li>实数编码</li>
<li>多参数映射编码（把每个参数先进行二进制编码得到子串，再把这些子串连成一个完整的染色体）</li>
</ul>
<h4 id="适应度计算方式"><a href="#适应度计算方式" class="headerlink" title="适应度计算方式"></a>适应度计算方式</h4><p>若目标函数为<strong>最大化</strong>问题，则$Fitness(f(x))=f(x)$。若目标函数为<strong>最小化</strong>问题，则$Fitness(f(x))=\frac 1 {f(x)}$。</p>
<p>在遗传算法中，将所有妨碍适应度值高的个体产生，从而影响遗传算法正常工作的问题统称为<strong>欺骗问题</strong>。常见的有过早收敛、停滞现象等。</p>
<p>解决方法有：</p>
<ul>
<li>缩小这些个体的适应度，以降低这些超级个体的竞争力；</li>
<li>或改变原始适应值对应的比例关系，以提高个体之间的竞争力；</li>
<li>或对适应度函数值域的某种映射变换。</li>
</ul>
<h4 id="选择子代方式"><a href="#选择子代方式" class="headerlink" title="选择子代方式"></a>选择子代方式</h4><p>常见的选择子代方式有；</p>
<ul>
<li>适应度比例方法：各个个体被选择的概率和其适应度值成比例。个体$i$被选择的概率为：$\quad p_{s i}=\frac{f_{i}}{\sum_{i=1}^{M} f_{i}}$​</li>
<li>排序方法：排序选择进入下一代的个体</li>
<li>赌盘轮转法：按个体的选择概率产生一个轮盘，轮盘每个区的角度与个体的选择概率成比例。产生一个随机数，它落入转盘的哪个区域就选择相应的个体。</li>
<li>锦标赛选择方法：从群体中随机选择个个体，将其中适应度最高的个体保存到下一代。</li>
</ul>
<h4 id="交叉方式"><a href="#交叉方式" class="headerlink" title="交叉方式"></a>交叉方式</h4><p>常见的交叉方式有；</p>
<ul>
<li>一点交叉：在个体串中随机设定一个交叉点，实行交叉时，该点前或后的两个个体的部分结构进行互换，并生成两个新的个体。</li>
<li>两点交叉：随机设置两个交叉点，将两个交叉点之间的码串相互交换。</li>
</ul>
<h1 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h1><h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><script type="math/tex; mode=display">
P(C|A) = \frac{P(A|C)P(C)}{P(A)}</script><p>其中：</p>
<ul>
<li>$P(C|A)$  是后验概率，即在已知特征A的情况下，样本属于类别C的概率。</li>
<li>$P(A|C)$ 是似然概率，即在已知类别C的情况下，观察到特征A的概率。</li>
<li>$P(C)$  是先验概率，即在没有观察到任何特征之前，样本属于类别C的概率。</li>
<li>$P(A)$  是边缘概率，即样本具有特征A的概率。</li>
</ul>
<h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>朴素贝叶斯分类器是贝叶斯分类器的一种，它假设所有特征在给定类别的条件下都是相互独立的。</p>
<script type="math/tex; mode=display">
P(A_1, A_2, ..., A_n | C) = P(A_1 | C)P(A_2 | C)...P(A_n | C)</script><p>朴素贝叶斯分类器通过以下方式从数据中估计概率：</p>
<ul>
<li>对于离散属性，使用类别下具有特定属性的样本数除以该类别的总样本数。</li>
<li>对于连续属性，可以离散化，也可以假设属性服从正态分布，并使用数据的均值和方差来估计分布的参数，然后利用这些参数来估计条件概率。</li>
</ul>
<h3 id="Laplace-平滑"><a href="#Laplace-平滑" class="headerlink" title="Laplace 平滑"></a>Laplace 平滑</h3><p>为了避免在计算概率时出现零概率的问题，可以使用Laplace平滑（也称为加一平滑）。</p>
<script type="math/tex; mode=display">
P(c) = \frac{N_c + 1}{N + K}</script><p>其中，$N_c$ 是类别c的样本数量，$N$ 是所有样本的总数，$K$ 是类别总数。通过添加1，可以避免计算得到的概率为零的情况。</p>
<p>对于条件概率，Laplace平滑也可以应用于特征值和类别组合的计数：</p>
<script type="math/tex; mode=display">
P(x|c) = \frac{N_{xc} + 1}{N_c + V}</script><p>其中，$N_{xc}$ 是在类别c下特征x取某个值的样本数量，$V$ 是特征x的所有可能值的数量。同样地，通过添加1，可以避免在训练集中没有出现过的特征值-类别组合导致概率为零的情况。</p>
<h3 id="分类过程"><a href="#分类过程" class="headerlink" title="分类过程"></a>分类过程</h3><p>给定一个新的样本，贝叶斯分类器通过计算其属于每个类别的后验概率$\frac{P(A|C)P(C)}{P(A)}$，然后选择具有最大后验概率的类别作为预测结果。</p>
<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>朴素贝叶斯的局限性在于它要求所有特征在给定类别的条件下都是相互独立的。而贝叶斯网络则考虑了不同特特征之间的依赖关系。</p>
<p>贝叶斯网络由以下两个部分组成：</p>
<ul>
<li>一个有向无环图</li>
<li>多个条件概率表</li>
</ul>
<p>一个贝叶斯网络如下：</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  flowchart LR
a((A))--&gt;b((B))--&gt;c((C))--&gt;d((D))--&gt;e((E))
  </pre></div>
<p>为了计算$P(A,B,C,D,E)$，就需要计算$P(E|A,B,C,D)P(D|A,B,C)P(C|A,B)P(B|A)P(A)$。计算这个式子比较复杂。</p>
<p>而又观察到，在B发生的情况下，A和C是条件独立的（A发不发生都不影响C，因为A只能通过影响B发生的概率影响C，而B已经发生）。所以可以化简$P(C|A,B)$为$P(C|B)$。</p>
<p>所以将原来的式子化简，可以得到$P(A,B,C,D,E)=P(E|D)P(D|C)P(C|B)P(B|A)P(A)$​，这样就简化了计算。</p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>基于因果关系的变量序列可以获得更加自然紧致的贝叶斯网络结构</strong></p>
</blockquote>
<h3 id="三类条件独立"><a href="#三类条件独立" class="headerlink" title="三类条件独立"></a>三类条件独立</h3><p>所以如果能找到条件独立的变量，就能进行化简。条件独立的变量有三类。</p>
<hr>
<h4 id="传递"><a href="#传递" class="headerlink" title="传递"></a>传递</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  flowchart LR
a((A))--&gt;b((B))--&gt;c((C))
  </pre></div>
<p>在B发生的条件下，A和C条件独立。</p>
<h4 id="共因"><a href="#共因" class="headerlink" title="共因"></a>共因</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart TB
a((A))--&gt;b((B)) &amp; c((C))
  </pre></div>
<p>当A发生的条件下，B和C条件独立。</p>
<blockquote>
<p>反过来说，如果A还不知道发不发生，B发生了。说明A很有可能发生，进而C有可能发生，因此不独立。</p>
</blockquote>
<h4 id="共果"><a href="#共果" class="headerlink" title="共果"></a>共果</h4><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart TB
a((A)) &amp; b((B)) --&gt; c((C))--&gt;d((D))
  </pre></div>
<p>A和B相互独立。但在C（或者其子节点D）发生的条件下，它们没有条件独立。</p>
<blockquote>
<p>如果C（或D）发生了，说明可能是A或B发生了。如果此时A没发生，那说明很可能是B发生了。</p>
</blockquote>
<hr>
<p>判断有向无环图中在特定条件下两个变量是否条件独立，可以看两个变量之间是否有连通的路（这里的路是无方向的）。<strong>连通不独立，独立不联通</strong>。</p>
<ul>
<li>传递链路上的事件发生，会阻塞路</li>
<li>共因的因发生，会阻塞路</li>
<li>共果的果发生，会连通路</li>
</ul>
<p>举一个例子：</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart LR
a((A))--&gt;b((B))--&gt;c((C))--&gt;d((D))
e((E))--&gt;a &amp; c
b &amp; g((G))--&gt;f((F))
  </pre></div>
<p>当C发生的时候，C阻塞了传递链路，因此E和D独立</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart LR
classDef red fill:#f96;
a((A))-.-&gt;b((B))-.-&gt;c((C))--&gt;d((D))
e((E))-.-&gt;a &amp; c
b &amp; g((G))--&gt;f((F))
class c red;
  </pre></div>
<p>当E和B发生的时候，E和B产生阻塞，因此A和D独立</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart LR
classDef red fill:#f96;
a((A))-.-&gt;b((B))--&gt;c((C))--&gt;d((D))
e((E))-.-&gt;a
e((E))--&gt;c
b &amp; g((G))--&gt;f((F))
class b red;
class e red;
  </pre></div>
<p>当F发生的时候，F是一个通路，因此B和G不独立</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart LR
classDef green fill:#90ee90;
a((A))--&gt;b((B))--&gt;c((C))--&gt;d((D))
e((E))--&gt;a
e((E))--&gt;c
b &amp; g((G))&#x3D;&#x3D;&gt;f((F))
class f green;
  </pre></div>
<h3 id="分类过程-1"><a href="#分类过程-1" class="headerlink" title="分类过程"></a>分类过程</h3><p>给定一个新的样本，贝叶斯网络也是通过计算其属于每个类别的后验概率$\frac{P(A|C)P(C)}{P(A)}$，然后选择具有最大后验概率的类别作为预测结果。只不过此时$P(A)$和$P(A|C)$​​在展开的时候用的不是相互独立而是条件独立去计算。</p>
<p>比如，在下面这个贝叶斯网络。计算$P(+b,+j,+m)$的时候就用条件独立代替了相互独立。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240420102307131.png" alt="image-20240420102307131"></p>
<script type="math/tex; mode=display">
\begin{aligned}
P(+b,+j,+m)= & P(+b) P(+e) P(+a \mid+b,+e) P(+j \mid+a) P(+m \mid+a)+ \\
& P(+b) P(+e) P(-a \mid+b,+e) P(+j \mid-a) P(+m \mid-a)+ \\
& P(+b) P(-e) P(+a \mid+b,-e) P(+j \mid+a) P(+m \mid+a)+ \\
& P(+b) P(-e) P(-a \mid+b,-e) P(+j \mid-a) P(+m \mid-a)
\end{aligned}</script><h4 id="变量消除"><a href="#变量消除" class="headerlink" title="变量消除"></a>变量消除</h4><p>如果直接枚举所有的情况，变量还是太多。这时候，就可以消除一些变量。</p>
<h5 id="合并因子"><a href="#合并因子" class="headerlink" title="合并因子"></a>合并因子</h5><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240420103339493.png" alt="image-20240420103339493"></p>
<p>比如果知道了$P(R)$和$P(T|R)$的概率表，就可以将其变为$P(R,T)$的概率表。</p>
<blockquote>
<p>如果$P(R)*P(T|R)$在一串式子中，那么两个乘积就可以变为一个乘积。</p>
</blockquote>
<h5 id="消除因子"><a href="#消除因子" class="headerlink" title="消除因子"></a>消除因子</h5><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240420105425960.png" alt="image-20240420105425960"></p>
<p>如果知道$R$不会以$-R$或$+R$的形式出现在表达式当中。就可以将这个因子消除。将$P(R,T)$的概率表变为$P(T)$的概率表。</p>
<h5 id="已知取值变量"><a href="#已知取值变量" class="headerlink" title="已知取值变量"></a>已知取值变量</h5><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240420110635467.png" alt="image-20240420110635467"></p>
<p>如果一个变量的取值已知，那它也可以被消除。</p>
<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h2><p>机器学习基本类别有两种：</p>
<ul>
<li>回归：函数的输出是一个数值</li>
<li>分类：函数的输出是一个类别</li>
</ul>
<p>这两类的目的都是找到一个最优的函数，而寻找函数的三个步骤是：</p>
<ol>
<li>确定候选函数的集合（Model）</li>
<li>确定「评价函数好坏」的标准（Loss）</li>
<li>找出最好的函数（最优化Optimization）</li>
</ol>
<p>根据学习模式对机器学习进行分类，机器学习可以分为：</p>
<ul>
<li>监督学习</li>
<li>无监督学习</li>
<li>半监督学习</li>
<li>强化学习</li>
</ul>
<h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><p>聚类算法目的是找到几个组，使组内相似度较高，而组间相似度较少。</p>
<p>聚类（clustering）是由一系列集群（clusters）组成的集合。其的类别有：</p>
<ul>
<li><p>分割式聚类：将数据对象划分为不重叠的子集(簇) ，这样每个数据对象都恰好在一个子集中。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240427160359945.png" alt="image-20240427160359945"></p>
</li>
<li><p>阶层式聚类：一组嵌套的集群，组织成一个层次树。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240427160411657.png" alt="image-20240427160411657"></p>
</li>
</ul>
<h3 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h3><p>算法流程：</p>
<ol>
<li>选择K个点作为初始的聚类中心</li>
<li>计算所有点到K个初始聚类中心的距离，每个点被划分到距离最近的聚类中心的那个簇。</li>
<li>重新计算每个簇的聚类中心（求坐标的平均值）</li>
<li>如果聚类中心没有变化，则算法结束，否则转2</li>
</ol>
<p>初始的聚类中心通常是随机选择的。也是因为随机选择，所以可能聚类效果并不好。</p>
<h4 id="算法评价方法"><a href="#算法评价方法" class="headerlink" title="算法评价方法"></a>算法评价方法</h4><p>可以用SSE（Sum of Squared Errors误差平方和）来评估聚类的质量。SSE 越小，表示数据点在聚类内部越紧密，聚类效果越好。</p>
<script type="math/tex; mode=display">
SSE=∑_{i=1}^{K} ∑_{x∈C_i} d^2(m_i, x)</script><p>其中，K 是聚类的数量，$C_i$ 是第 $i$ 个聚类，$m_i$ 是第 $i$ 个聚类的中心点，$x$ 是聚类 $C_i$ 中的一个数据点，$d(m_i, x)$ 是点 $x$ 到中心点 $m_i$ 的距离。</p>
<h4 id="K-means算法的改进"><a href="#K-means算法的改进" class="headerlink" title="K-means算法的改进"></a>K-means算法的改进</h4><h5 id="K-means"><a href="#K-means" class="headerlink" title="K-means++"></a>K-means++</h5><p>改进了初始选择聚类中心的方法：</p>
<ol>
<li>第一个聚类中心随机选取</li>
<li>以每个点分别为$\frac{D\left(x^{\prime}\right)^{2}}{\sum_{x \in \mathcal{X}} D(x)^{2}}$的概率选择下一个聚类中心。$D(x)$是数据点$x$到已经选择的最近的聚类中心的距离，也就是会选尽量与已选择的聚类中心较远的点作为聚类中心。</li>
<li>如果没选择到K个聚类中心则转2。</li>
</ol>
<p>但其也有缺点：每次选择聚类中心都需要遍历一次剩下的点，一共K次遍历。</p>
<h5 id="K-means-1"><a href="#K-means-1" class="headerlink" title="K-means||"></a>K-means||</h5><p>K-means||也叫稳定的K-means。</p>
<p>主要思路在于改变每次遍历时的取样策略，并非按照 kmeans++ 那样每次遍历只取样一个样本，而是每次遍历取样 $O(k)$ 个样本，重复该取样过程大约 $O(logn)$ 次，共得到 $O(klogn)$ 个样本点组成的集合。然后再聚类这 $O(klogn)$ 个点成 $k$ 个点，最后将这 $k$ 个点作为初始聚类中心。</p>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><p>DBSCAN是一个基于密度的算法。（密度：指定半径内的点数）</p>
<p>给定$Eps$作为半径、$MinPts$作为最小的点的数量，其将点分为三类：</p>
<ul>
<li><p>核心点（core point）：在$Eps$（半径范围内）就有超过指定数量（$MinPts$）的点。</p>
</li>
<li><p>边界点（border point）：在$Eps$内少于$MinPts$个点，但它位于一个核心点的邻域内</p>
</li>
<li>噪声点（noise point）：其他点。</li>
</ul>
<p>算法流程：</p>
<ol>
<li>对所有点进行标记，核心点、边界点或噪声点</li>
<li>消除所有的噪声点。</li>
<li>对于所有核心点，如果在$Eps$范围内，则它们之间添加一条边。</li>
<li>将相连的核心点组成不同的簇</li>
<li>将每个边界点分配到它相关的核心点的某个簇中</li>
</ol>
<p>但其对$Eps$和$MinPts$的选择有要求。</p>
<h3 id="评价方法"><a href="#评价方法" class="headerlink" title="评价方法"></a>评价方法</h3><ol>
<li>相似度矩阵</li>
<li>SSE（凝聚度）和SSB（分离度）两者之和是定值（TSS）<ul>
<li>$SSE = \sum_{i=1}^{k} \sum_{x \in C_i} | x - m_i |^2$ </li>
<li>$SSB = \sum_{i=1}^{k} n_i | m_i - m |^2$。其中，$k$ 是聚类的数量，$n_i$ 是第 $i$个聚类中的点的数量，$m_i$ 是第 $i$个聚类的中心点，$m$是所有数据点的中心点（通常是所有点的均值）。</li>
</ul>
</li>
</ol>
<h3 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29849122">参考文章</a></p>
<p>谱聚类(Spectral Clustering)要求比K-means少，结果比K-means好，复杂度比K-means小。</p>
<h4 id="图的一些概念"><a href="#图的一些概念" class="headerlink" title="图的一些概念"></a>图的一些概念</h4><ul>
<li><p>$|A|$：图中顶点的个数</p>
</li>
<li><p>$vol(A)$：图的所有顶点的度数之和</p>
</li>
<li><p>相似度矩阵：</p>
<ul>
<li>$A(i, j)=0。如果i,j不相邻$</li>
<li>$A(i, j)=s(i, j)。如果i,j相邻$</li>
</ul>
</li>
<li><p>度矩阵：</p>
<ul>
<li><p>$D(i, j)=0。如果i\neq j$</p>
</li>
<li><p>$D(i,j)=d(v_i)。如果i=j相邻$​</p>
<blockquote>
<p>对角线上是每个节点自己的度</p>
</blockquote>
</li>
</ul>
</li>
<li><p>拉普拉斯矩阵：$L=D-W$​，其有如下性质：</p>
<blockquote>
<p>D是度矩阵，W是邻接矩阵</p>
</blockquote>
<ul>
<li><p><strong>对称性</strong>：由于D和W都是对称的，L也是对称的。</p>
</li>
<li><p><strong>半正定性</strong>：拉普拉斯矩阵的所有 eigenvalues（特征值）都是非负的，这意味着对于所有的向量$x，x^T L x ≥ 0$。</p>
</li>
<li><p><strong>最小特征值</strong>：拉普拉斯矩阵的最小特征值是0，对应的特征向量是所有元素都是1的常数向量。</p>
</li>
<li><p><strong>特征值排序</strong>：拉普拉斯矩阵的特征值可以按照非递减的顺序排列：$0 = λ_1 ≤ λ_2 ≤ … ≤ λ_N$。</p>
</li>
<li><p><strong>特征值之间的差距</strong>：最小非零特征值$λ_2$被称为谱隙（spectral gap），它度量了图结构的扩展性。</p>
</li>
<li><p><strong>特征向量的正交性</strong>：拉普拉斯矩阵有一组完整的正交特征向量。</p>
</li>
<li><p>对于任意的向量有</p>
<script type="math/tex; mode=display">
f^{T} L f=\frac{1}{2} \sum_{i, j=1}^{n} w_{i j}\left(f_{i}-f_{j}\right)^{2}</script></li>
</ul>
</li>
</ul>
<h4 id="图分割"><a href="#图分割" class="headerlink" title="图分割"></a>图分割</h4><p>图分割要求把一个图分割为几个子图，并使得那些被切断的边的权值之和最小。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240429203641057.png" alt="image-20240429203641057"></p>
<h5 id="无向图切图"><a href="#无向图切图" class="headerlink" title="无向图切图"></a>无向图切图</h5><h6 id="Cut"><a href="#Cut" class="headerlink" title="Cut"></a>Cut</h6><p>对于任意两个子图点的集合 $A, B \subset V$，其中 $A \cap B=\emptyset$，定义 $A$ 和 $B$ 之间的切图权重为：</p>
<script type="math/tex; mode=display">
W(A, B)=\sum_{i \in A, j \in B} w_{ij}</script><p>其中 $w_{ij}$ 表示点 $i$ 和点 $j$ 之间的权重。</p>
<p>对于 $k$ 个子图点的集合 $A_{1}, A_{2}, \ldots A_{k}$，定义切图Cut为：</p>
<script type="math/tex; mode=display">
\operatorname{cut}(A_{1}, A_{2}, \ldots A_{k})=\frac{1}{2} \sum_{i=1}^{k} W\left(A_{i}, \bar{A}_{i}\right)</script><p>这里，$\bar{A}_{i}$ 表示 $A_{i}$ 的补集，即除 $A_{i}$ 子集外其他 $V$ 的子集的并集。</p>
<blockquote>
<p>一个好的切图，要让切图Cut最小</p>
</blockquote>
<p>但是仅使用这个Cut定义，会得到不够好的切图：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240429211438139.png" alt="image-20240429211438139"></p>
<h6 id="RatioCut"><a href="#RatioCut" class="headerlink" title="RatioCut"></a>RatioCut</h6><p>RatioCut切图对每个切图，不光考虑最小化$\operatorname{cut}(A_{1}, A_{2}, \ldots A_{k})$ ，它还同时考虑最大化每个子图点的个数：</p>
<script type="math/tex; mode=display">
\operatorname{RatioCut}\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\left|A_{i}\right|}</script><p>而要最小化RatioCut，需要引入指示向量：$h_{j} \in\left\{h_{1}, h_{2}, . . h_{k}\right\} ,j=1,2, \ldots k$，它是一个$n$维向量。$h_{ij}$定义为：</p>
<script type="math/tex; mode=display">
h_{i j}=\left\{\begin{array}{ll}
0 & v_{i} \notin A_{j} \\
\frac{1}{\sqrt{\left|A_{j}\right|}} & v_{i} \in A_{j}
\end{array}\right.</script><p>意思是如果点$v_i$属于$A_j$集合，则$h_{ij}=\frac{1}{\sqrt{\left|A_{j}\right|}}$。</p>
<p>有了指示向量后，RatioCut就可以化简为：</p>
<script type="math/tex; mode=display">
\operatorname{RatioCut}\left(A_{1}, A_{2}, \ldots A_{k}\right)=\sum_{i=1}^{k} h_{i}^{T} L h_{i}=\sum_{i=1}^{k}\left(H^{T} L H\right)_{i i}=\operatorname{tr}\left(H^{T} L H\right)</script><p>要最小化RatioCut，需要优化每一个$h_{i}^{T} L h_{i}$。其中 $h$ 是单位正交基， $L$ 为对称矩阵，此时 $h^T L h$ 的最大值为 $L$ 的最大特征值，最小值是 $L$ 的最小特征值。</p>
<p>所以通过找到$L$的最小的$k$个特征值，可以得到对应的$k$个特征向量，这$k$个特征向量组成一个$n\cdot k$维度的矩阵，即为$H$​。</p>
<p>由于在使用维度规约的时候损失了少量信息，导致得到的优化后的指示向量$h$对应的$H$现在不能完全指示各样本的归属。</p>
<blockquote>
<p>如果没有损失信息，$h_{ij}=\frac{1}{\sqrt{\left|A_{j}\right|}}$代表点$v_i$属于$A_j$集合。</p>
</blockquote>
<p>因此一般在得到$n\cdot k$维度的矩阵$H$后还需要对每一行进行一次传统的聚类，比如使用K-Means聚类。</p>
<h6 id="NCut"><a href="#NCut" class="headerlink" title="NCut"></a>NCut</h6><p>Ncut切图和RatioCut切图很类似，但是把RatioCut的分母 $|A_i|$ 换成 $\text{vol}(A_i)$：</p>
<script type="math/tex; mode=display">
\operatorname{NCut}\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\operatorname{vol}\left(A_{i}\right)}</script><p>同样选取指示向量：</p>
<script type="math/tex; mode=display">
h_{i j}=\left\{\begin{array}{ll}
0 & v_{i} \notin A_{j} \\
\frac{1}{\sqrt{\operatorname{vol}\left(A_{j}\right)}} & v_{i} \in A_{j}
\end{array}\right.</script><p>将优化目标转换为：</p>
<script type="math/tex; mode=display">
\operatorname{NCut}\left(A_{1}, A_{2}, \ldots A_{k}\right)=\sum_{i=1}^{k} h_{i}^{T} L h_{i}=\sum_{i=1}^{k}\left(H^{T} L H\right)_{i i}=\operatorname{tr}\left(H^{T} L H\right)</script><p>此时$H$中的指示向量$h$并不是标准正交基，而是$H^{T} D H=I$，所以在RatioCut里面的降维思想不能直接用。</p>
<p>令$H=D^{-1 / 2} F$,则$H^{T} L H=F^{T} D^{-1 / 2} L D^{-1 / 2} F$。此时$H^{T} D H=F^{T} F=I$。可以继续按照RatioCut的方法降维，求出 $D^{-1/2} L D^{-1/2}$ 的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，得到最后的特征矩阵 $F$，最后对 $F$ 进行一次传统的聚类（比如K-Means）即可。</p>
<h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><h5 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h5><p>用于构建图的方法有：</p>
<ul>
<li>全连接图</li>
<li>$r$-邻接图（结点与以其为圆心，$r$为半径内的其他结点相连）</li>
<li>$k$-最近邻图（结点与最近的$k$个邻居相连）</li>
</ul>
<h5 id="构建拉普拉斯矩阵"><a href="#构建拉普拉斯矩阵" class="headerlink" title="构建拉普拉斯矩阵"></a>构建拉普拉斯矩阵</h5><p>根据相似矩阵$S$构建邻接矩阵$W$，构建度矩阵$D$。然后计算拉普拉斯矩阵$L=D-W$​。</p>
<h5 id="根据切图方法计算特征值"><a href="#根据切图方法计算特征值" class="headerlink" title="根据切图方法计算特征值"></a>根据切图方法计算特征值</h5><p>以下$k_1$为降维后的维度</p>
<ul>
<li>RatioCut计算 $L$ 的最小的前$k_1$个特征值</li>
<li>NCut计算$D^{-1/2} L D^{-1/2}$ 的最小的前$k_1$​​个特征值</li>
</ul>
<h5 id="计算特征矩阵"><a href="#计算特征矩阵" class="headerlink" title="计算特征矩阵"></a>计算特征矩阵</h5><p>将各自对应的特征向量$f$组成的矩阵按行标准化，最终组成$n\cdot k_1$维的特征矩阵$F$​。</p>
<h5 id="对特征矩阵聚类"><a href="#对特征矩阵聚类" class="headerlink" title="对特征矩阵聚类"></a>对特征矩阵聚类</h5><p>对$F$中的每一行作为一个$k_1$维的样本，共$n$个样本，用输入的聚类方法（比如K-means）进行聚类，聚类维数为$k_2$（划分为$k_2$个簇）。</p>
<p>最后得到簇划分 $C(c_1, c_2, \ldots, c_{k})$。</p>
<h2 id="SVM-不考"><a href="#SVM-不考" class="headerlink" title="SVM(不考)"></a>SVM(不考)</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/7624837">参考文章</a>，这篇博客写的非常透彻，强推。</p>
</blockquote>
<p>SVM（Support Vector Machine）也称支持向量机，是一个将数据单元表示在多维空间中，然后对这个空间做划分的算法。而支持向量指的是支撑平面上把两类类别划分开来的超平面的向量点。</p>
<p>SVM在解决<strong>小样本</strong>、<strong>非线性</strong>、及<strong>高维模式识别</strong>中有很多优势。</p>
<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><h4 id="结构风险"><a href="#结构风险" class="headerlink" title="结构风险"></a>结构风险</h4><p>结构风险 = 经验风险 + 置信风险。</p>
<ul>
<li><strong>经验风险</strong>：在给定样本上的误差。</li>
<li><strong>置信风险</strong>：在多大程度上可以信任分类器在未知样本上分类的结果。</li>
</ul>
<p>SVM 的目标是最小化结构风险，而不再只是优化经验风险，否则会过拟合。</p>
<h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p>几何间隔，指的是点到超平面距离（不同于点到平面的距离）：</p>
<script type="math/tex; mode=display">
\tilde{\gamma}=y \gamma=\frac{\hat{\gamma}}{\|w\|}=\frac{y\left(w^{T} x+b\right)}{\|w\|}</script><p>其中，超平面为$y=\omega^T x+b$。$\hat y$是函数间隔。</p>
<h3 id="SVM目标函数"><a href="#SVM目标函数" class="headerlink" title="SVM目标函数"></a>SVM目标函数</h3><h4 id="原问题"><a href="#原问题" class="headerlink" title="原问题"></a>原问题</h4><p> 对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的效果就越好。所以对所有点来说，应该使得平面恰好夹在两类点的中间。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240516101840880.png" alt="image-20240516101840880"></p>
<p>所以就需要使最靠近分界线的点离分界线的几何间隔最大。所以目标函数定义为：</p>
<script type="math/tex; mode=display">
\max \tilde{\gamma}\\约束条件:y_{i}\left(w^{T} x_{i}+b\right)=\hat{\gamma}_{i} \geq \hat{\gamma}, \quad i=1, \ldots, n</script><p>如果令函数间隔 $\hat{\gamma}$​ 等于1（方便推导和优化，且这样做对目标函数的优化没有影响）。则目标问题变为：</p>
<script type="math/tex; mode=display">
\max \frac{1}{\|w\|} \\约束条件:y_{i}\left(w^{T} x_{i}+b\right) \geq 1, i=1, \ldots, n</script><p>由于求 $\frac{1}{|w|}$ 的最大值相当于求 $\frac{1}{2}|w|^{2}$ 的最小值，所以可以将目标函数转换为：</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}\|w\|^{2} \\约束条件:y_{i}\left(w^{T} x_{i}+b\right) \geq 1, i=1, \ldots, n</script><p> 因为原问题的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题，有全局的最优解。</p>
<h4 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h4><h5 id="拉格朗日变换"><a href="#拉格朗日变换" class="headerlink" title="拉格朗日变换"></a>拉格朗日变换</h5><p>由于原问题的特殊结构，还可以通过<strong>拉格朗日对偶性</strong>变换为对对偶问题的求解。</p>
<p>通过给每一个约束条件加上一个拉格朗日乘子$\alpha$，可以将约束条件融合到目标函数里去，从而将约束优化问题转换为无约束优化问题：</p>
<script type="math/tex; mode=display">
\mathcal{L}(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)</script><p>并令：</p>
<script type="math/tex; mode=display">
\theta(w)=\max _{\alpha_{i} \geq 0} \mathcal{L}(w, b, \alpha)</script><p>而当所有约束条件都满足时，则最优值为$\theta(w)=\frac{1}{2}|w|^{2}$。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}|w|^{2}$，实际上等价于直接最小化 $\theta(w)$。</p>
<p>用$p^*$​表示问题的最优值，则目标函数变为：</p>
<script type="math/tex; mode=display">
\min _{w, b} \theta(w)=\min _{w, b} \max _{\alpha_{i} \geq 0} \mathcal{L}(w, b, \alpha)=p^{*}</script><p>这个问题的求解较为困难（因为有参数$w、b$，并且$a_i$​​还受到不等式约束）。<strong>所以把最小和最大的位置交换一下，得到原问题的对偶问题：</strong></p>
<script type="math/tex; mode=display">
\max _{\alpha_{i} \geq 0} \min _{w, b} \mathcal{L}(w, b, \alpha)=d^{*}</script><p>$d^{<em>}$是新问题的最优解，并且有$d^{</em>}\leq p^{<em>}$。并且因为Slater 条件成立，所以$d^{</em>}\leq p^{*}$​可以取等号。</p>
<h5 id="求解对偶问题"><a href="#求解对偶问题" class="headerlink" title="求解对偶问题"></a>求解对偶问题</h5><p>首先固定 $\alpha$ ，要让 $\mathcal{L}$ 关于 $w$ 和 $b$ 最小化 我们分别对 $w, b$ 求偏导数，即令 $\partial \mathcal{L} / \partial w$ 和 $\partial \mathcal{L} / \partial b$ 等于零。 可以得到：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial \mathcal{L}}{\partial w}=0 \Rightarrow w=\sum_{i=1}^{n} \alpha_{i} y_{i} x_{i} \\
\frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow \sum_{i=1}^{n} \alpha_{i} y_{i}=0
\end{array}</script><p>将以上结果代入之前的$\mathcal{L}$，化简后得到：</p>
<script type="math/tex; mode=display">
\mathcal{L}(w, b, \alpha)==\sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}</script><p>此时的拉格朗日函数只包含了一个变量$\alpha_i$。然后就可以根据下式求解出$\alpha_i$：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
&\max _{\alpha} \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{n} \alpha_{i} \alpha_{j} y_{j} y_{j} x_{i}^{T} x_{j} \\
\text {约束条件:}&\alpha_{i} \geq 0, i=1, \ldots, n \\
&\sum_{i=1}^{n} \alpha_{i} y=0
\end{array}</script><p>然后就可以根据$w=\sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}$，求解出$\omega$；根据$b^{<em>}=-\frac{\max _{i: y^{(i)}=-1} w^{</em> T} x^{(i)}+\min _{i: y^{(i)}=1} w^{* T} x^{(i)}}{2}$求解出$b$​。最终得出分离超平面和分类决策函数。</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><h4 id="SVM分类过程"><a href="#SVM分类过程" class="headerlink" title="SVM分类过程"></a>SVM分类过程</h4><p>对于一个数据点 $x$ 进行分类, 实际上是通过把 $x$ 带入到 $f(x)=w^{T} x+b$ 算出结果然后根据其正负号来进行类别划分的。</p>
<p>而$w=\sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}$，所以分类函数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f(x) & =\left(\sum_{i=1}^{n} \alpha_{i} y_{i} x_{i}\right)^{T} x+b \\
& =\sum_{i=1}^{n} \alpha_{i} y_{i}\left\langle x_{i}, x\right\rangle+b
\end{aligned}</script><p><strong>那么，对于新点$x$​的预测，只需要计算它与训练数据点的内积即可。</strong></p>
<blockquote>
<p>事实上，所有非支持向量所对应的系数$\alpha$​都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”，而不是所有的训练数据。</p>
</blockquote>
<h4 id="核函数的作用"><a href="#核函数的作用" class="headerlink" title="核函数的作用"></a>核函数的作用</h4><p>对于非线性的情况，SVM 的处理方法是选择一个核函数 $κ(⋅,⋅)$​ ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。</p>
<blockquote>
<p>而在核函数之前，如果用原始的方法，在用线性学习器学习一个非线性关系时，需要选择一个非线性特征集，并且将数据写成新的表达形式，这等价于应用一个固定的非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器。</p>
</blockquote>
<p>如果将原来的分类函数</p>
<script type="math/tex; mode=display">
f(x)=\sum_{i=1}^{n} \alpha_{i} y_{i}\left\langle x_{i}, x\right\rangle+b</script><p>直接找一个映射 $\phi(\cdot)$，把原来的数据映射到新空间中, 再做线性SVM。就会有维度爆炸的问题。</p>
<script type="math/tex; mode=display">
f(x)=\sum_{i=1}^{n} \alpha_{i} y_{i}\left\langle\phi\left(x_{i}\right), \phi(x)\right\rangle+b</script><p>所以相比映射到高维空间中，然后再根据内积的公式进行计算。核函数选择另一个方式：<strong>直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果</strong>。</p>
<h4 id="核函数的定义"><a href="#核函数的定义" class="headerlink" title="核函数的定义"></a>核函数的定义</h4><p>核函数是一个函数 $\kappa$，对所有 $x, z \in X$，满足 $\kappa(\mathbf{x}, \mathbf{z}) = \langle\phi(\mathbf{x}) \cdot \phi(\mathbf{z})\rangle$，这里 $\phi$ 是从 $X$ 到内积特征空间 $F$ 的映射。</p>
<p><strong>计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数。</strong>它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就避免了直接在高维空间中的复杂计算。</p>
<h4 id="常见的核函数"><a href="#常见的核函数" class="headerlink" title="常见的核函数"></a>常见的核函数</h4><p>常见的核函数有：</p>
<ul>
<li>多项式核：$\kappa(x, y)=\left(\alpha x^{T} y+c\right)^{d}$</li>
<li>高斯核：$\kappa(x, y)=\exp \left(-\gamma|x-y|^{2}\right)$</li>
<li>线性核 ：$\kappa\left(x, y\right)=\left\langle x, y\right\rangle$</li>
</ul>
<h3 id="松弛变量处理离群点"><a href="#松弛变量处理离群点" class="headerlink" title="松弛变量处理离群点"></a>松弛变量处理离群点</h3><p>虽然通过映射 $\phi(\cdot)$ 将原始数据映射到高维空间之后，能够线性分隔的概率大大增加，但是对于某些情况还是很难处理。</p>
<p>例如一些离群点（偏离正常位置很远的数据点），会对SVM模型造成很大影响。</p>
<blockquote>
<p>因为超平面本身就是只有少数几个支持向量组成的，如果这些支持向量里又存在离群点的话，其影响就很大了</p>
</blockquote>
<p>通过向约束条件中加入松弛变量$\xi_{i}$：</p>
<script type="math/tex; mode=display">
y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}, \quad i=1, \ldots, n</script><p>允许数据点离超平面更近。并且向目标函数中加入惩罚项，限制这种松弛：</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{n} \xi_{i}</script><p>再进行上面的一系列操作（拉格朗日对偶、核函数），就可以得到一个完整的、可以处理线性和非线性、并能容忍噪音和离群点的支持向量机。</p>
<h2 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h2><h3 id="发展史"><a href="#发展史" class="headerlink" title="发展史"></a>发展史</h3><h4 id="MP模型"><a href="#MP模型" class="headerlink" title="MP模型"></a>MP模型</h4><p>最初提出的神经网络模型是MP（McCulloch-Pitts ）模型，其的权重是预先设置的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240427181414661.png" alt="image-20240427181414661"></p>
<h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><p>之后发展出了感知机，也被称为单层神经网络，其的权重是通过训练得到的，训练法则为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
W_{t} \leftarrow W_{t}+\Delta W_{t} \\
\Delta W_{t}=\eta(y-\hat{y}) \boldsymbol{a}
\end{array}</script><p>其$f$神经元中执行的是线性运算$\mathbf{z}=g(\boldsymbol{W} \times \boldsymbol{a})$，所以只能执行线性分类。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240427181445146.png" alt="image-20240427181445146"></p>
<p>也是因为神经元执行的是线性运算，所以<strong>感知机只能做简单的线性分类任务</strong>，无法解决XOR（异或）问题。</p>
<h4 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h4><p>之后就发展出了多层感知机（多层的神经网络）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240427181959937.png" alt="image-20240427181959937"></p>
<p>并且其不再使用sgn函数做激活函数，而是使用sigmoid函数</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240427182037505.png" alt="image-20240427182037505"></p>
<p>关键在于: 从输入层到隐藏层时，<strong>数据在激活函数作用下发生了空间变换</strong></p>
<p>其通过前向传播和反向传播来进行训练，前向传播为了计算当前参数下的误差，反向传播是为了根据误差调整参数。反向传播常用的算法是梯度下降。</p>
<h3 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h3><p>神经网络的一个神经元的基础操作是对输入$x$执行：</p>
<ol>
<li>线性运算：$z=W x+b$​</li>
<li>通过激活函数：$y=\sigma(z)$</li>
</ol>
<p>从功能角度讲，神经网络就是多层的<strong>特征提取</strong>加上<strong>线性分类</strong>。</p>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h4 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h4><p>人观测图片的特点： <strong>① 局部特征影响大 ② 重要位置常变化</strong> <strong>③ 采样压缩也没差</strong></p>
<p>根据前两个特点特点，设计卷积神经网络，就设计出了可以移动的卷积核。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240511163332296.png" alt="image-20240511163332296"></p>
<p>卷积过程中的步长（stride）指的是卷积核在输入图像上每次移动的像素数量；卷积操作前的填充（padding）操作，指的是在输入图像的边界周围添加额外的像素。</p>
<h5 id="卷积维度公式"><a href="#卷积维度公式" class="headerlink" title="卷积维度公式"></a>卷积维度公式</h5><p>假设图片尺寸为 $n_{h} \times n_{w} \times n_{c}$，卷积核为 $f \times f \times n_{c}$，步长 $s$，填充 $p$​。其中：</p>
<ul>
<li><p>$n_{h}$ 是高度，$n_{w}$ 是宽度，$n_{c}$ 是深度（通道数）</p>
</li>
<li><p>$f$ 是卷积核的尺寸，$n_{c}$​ 是卷积核的深度（与输入图片的深度相同）</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240511164341249.png" alt="image-20240511164341249"></p>
<p>那么特征图（卷积层输出）维度公式：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\text { 高度 } h=\left\lfloor\frac{n_{h}-f+2 p}{s}+1\right\rfloor \\
\text { 宽度 } w=\left\lfloor\frac{n_{w}-f+2 p}{s}+1\right\rfloor \\
\text { 宽度 } k=K
\end{array}</script><h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><p>池化操作将特征图划分为若干个子区域，并对每个子区域进行统计汇总。</p>
<p>池化操作的方式可以有很多种，比如最大池化、平均池化等。</p>
<ul>
<li>最大池化：选取每个子区域内的最大值作为输出</li>
<li>平均池化：计算每个子区域内的平均值作为输出</li>
</ul>
<h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>循环神经网络要解决的槽填充问题：</p>
<p>I would like to arrive <u>Guangzhou</u> on <u>May</u>.</p>
<p>CNN和普通的神经网络都没有记忆功能，而RNN可以记忆。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240511183046525.png" alt="image-20240511183046525"></p>
<h3 id="长短期记忆网络（LSTM）"><a href="#长短期记忆网络（LSTM）" class="headerlink" title="长短期记忆网络（LSTM）"></a>长短期记忆网络（LSTM）</h3><p>RNN网络没有遗忘的功能，所以以往的所有输入都会影响输出，而LSTM加入了遗忘功能。它用特殊的神经元代替了普通神经网络中的神经元。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240511183946686.png" alt="image-20240511183946686"></p>
<p>这个特殊的神经网络有三个门：</p>
<ul>
<li>输出门：控制是否输出</li>
<li>遗忘门：选择是否遗忘</li>
<li>输入门：控制是否输入</li>
</ul>
<p>一份输入要充当这三个门的信号和神经元的输入。</p>
<p>缺点是参数量大了，训练和计算时间也长了。</p>
<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>强化学习是通过与环境进行交互、试错，从而学会做出一系列好的决策。</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
  %%{init:{&#39;flowchart&#39;:{&#39;curve&#39;:&#39;basis&#39;}}}%%
flowchart TD
a[Agent]--Action--&gt;e[Environment]
e--Reward--&gt;a
e--state--&gt;a
  </pre></div>
<p>强化学习的特殊之处在于：</p>
<ul>
<li><p>环境刚开始是未知的</p>
</li>
<li><p>智能体要与环境互动</p>
</li>
</ul>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="随机性的两个来源"><a href="#随机性的两个来源" class="headerlink" title="随机性的两个来源"></a>随机性的两个来源</h4><p>随机性的来源有两个：</p>
<ul>
<li><p>给定状态，智能体的行为具有随机性，由策略来决定。</p>
<script type="math/tex; mode=display">
A \sim \pi(\cdot \mid s)</script><p><strong>其中$\pi$是策略</strong>，$\pi(a_1|s)$代表的是在状态$s$下选择行为$a_1$的概率。</p>
</li>
<li><p>状态的转移也具有随机性，由状态转移函数来决定。</p>
<script type="math/tex; mode=display">
S^{\prime} \sim p(\cdot \mid s, a)</script><p>其中$S’$是次态，$p(\cdot \mid s, a)$指的是在状态$s$和行为$a$下转移到次态$S’$的概率。</p>
</li>
</ul>
<h4 id="AI玩游戏"><a href="#AI玩游戏" class="headerlink" title="AI玩游戏"></a>AI玩游戏</h4><p>可以将AI玩游戏的过程看成一个由状态（$s$）、行为（$a$）和奖励（$r$）组成的序列。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240523192432446.png" alt="image-20240523192432446"></p>
<p>一个完整的交互序列（episode），就是从初始状态开始到最终状态结束。一个好的策略能让最后<strong>积累</strong>的奖励最大。</p>
<h4 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h4><p>回报（Return）即累计未来奖励。假设$R_t$是$t$时刻的奖励，那么回报$U_t$的定义为：</p>
<script type="math/tex; mode=display">
U_{\mathrm{t}}=R_{\mathrm{t}}+\gamma R_{\mathrm{t}+1}+\gamma^{2} R_{\mathrm{t}+2}+\cdots+\gamma^{\mathrm{n}-\mathrm{t}} R_{\mathrm{n}} .</script><p>因为在$t$时刻，未来的奖励具有随机性，所以$U_t$也具有随机性。并且因为奖励的越早，奖励越具有吸引力。所以随着获得奖励所需的时间越久，其的价值也越低，在公式中以衰减因子$\gamma$表示。</p>
<h4 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h4><h5 id="行为价值函数"><a href="#行为价值函数" class="headerlink" title="行为价值函数"></a>行为价值函数</h5><p>行为价值函数$Q_\pi(s,a)$是针对策略$\pi$而言的。其的定义如下：</p>
<script type="math/tex; mode=display">
Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right]</script><p>$Q_{\pi}\left(s_{t}, a_{t}\right)$代表的是，在$\pi$策略下，在$s_t$的状态下、选择行动$a_t$，获得回报的期望。</p>
<p>可以定义最优行为价值函数为：</p>
<script type="math/tex; mode=display">
Q^{\star}\left(s_{t}, a_{t}\right)=\max _{\pi} Q_{\pi}\left(s_{t}, a_{t}\right)</script><p>其代表着在$s_t$的状态下、选择行动$a_t$，在所有策略中，能获得的最大的回报的期望。</p>
<h5 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h5><p>状态价值函数$V_\pi(s_t)$的定义如下：</p>
<script type="math/tex; mode=display">
V_{\pi}\left(s_{t}\right)=\mathbb{E}_{A}\left[Q_{\pi}\left(s_{t}, A\right)\right]=\sum_{a} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right)</script><p>$V_\pi(s_t)$代表的是，在$\pi$策略下，在$s_t$的状态下，获得回报的期望。</p>
<blockquote>
<p>状态价值函数和行为无关。</p>
</blockquote>
<p>可以用$\mathbb{E}_{S}\left[V_{\pi}(S)\right]$来衡量策略$\pi$有多棒。</p>
<h3 id="进行强化学习"><a href="#进行强化学习" class="headerlink" title="进行强化学习"></a>进行强化学习</h3><p>单智能体强化学习问题可以形式化定义为马尔可夫决策过程（MDP）六元组：</p>
<script type="math/tex; mode=display">
\left(S, A, R, T, P_{0}, \gamma\right)</script><p>其中：</p>
<ul>
<li>$S$ 表示状态空间，</li>
<li>$\boldsymbol{A}$ 表示动作空间，</li>
<li>$R=R(S, \boldsymbol{a})$ 表示奖励函数，</li>
<li>$T: S \times \boldsymbol{A} \times S \rightarrow [\mathbf{0}, \mathbf{1}]$ 表示状态转移函数，</li>
<li>$\boldsymbol{P}_{0}$ 表示初始状态分布，</li>
<li>$\gamma$ 是折扣因子。</li>
</ul>
<p>强化学习的学习目标: <strong>找到能最大化累计奖励的策略$\pi$​</strong>。</p>
<hr>
<p>有这几种学习方式：</p>
<ul>
<li>基于价值（Value-based learning）：它的目标是学习出一个价值函数，这个价值函数能够评估给定状态下采取某个动作的期望回报。</li>
<li>基于策略（Policy-based learning）：它直接学习策略函数，这个策略函数能够根据当前状态直接输出一个动作分布或确定性的动作。</li>
<li>Actor-critic方法：它结合了Value-based learning和Policy-based learning两种方法的优点。</li>
</ul>
<h4 id="基于价值学习"><a href="#基于价值学习" class="headerlink" title="基于价值学习"></a>基于价值学习</h4><p>基于价值学习需要近似$Q^{\star}\left(s_{t}, a_{t}\right)$，那么就可以知道每一个行动$a_t$能带来多大的价值。近似所用的方法是DQN（Deep Q-Network），即深度Q网络。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240524094425103.png" alt="image-20240524094425103"></p>
<p>但在训练DQN的时候，和训练任何一个神经网络一样，需要给出标签值，也就是$a_t$实际的价值是多少，然后与预测出来的$a_t$的价值去求损失。但直接求实际价值又太慢了（需要完整的完成一个迭代），所以就有了TD学习（Temporal Difference (TD) Learning）。</p>
<p>TD学习将实际价值近似为「当前奖励+下一步预测值」，只关注时间差分，即当前时间步的预测值和下一时间步的预测值之间的差异。数学的表达形式为：</p>
<script type="math/tex; mode=display">
Q\left(s_{t}, a_{t} ; \mathbf{w}\right) \approx r_{t}+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)</script><p>右边就是「当前奖励+下一步预测值」（还乘了损失因子），用其来近似实际价值。然后就可以用这个近似的实际价值当做神经网络的标签，去进行训练。</p>
<h4 id="基于策略学习"><a href="#基于策略学习" class="headerlink" title="基于策略学习"></a>基于策略学习</h4><p>基于策略学习和基于价值学习的方法类似，它也是用神经网络去近似策略函数$\pi(a|s)$。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240524102021748.png" alt="image-20240524102021748"></p>
<p>它的目的是优化${\theta}$使得$J({\theta})=\mathbb{E}_{S}[V(S ; {\theta})]$​最大。也就是让所有状态下采取给定策略得到的回报期望最大。这个回报期望可以通过跑完全程得到，也可以通过训练一个神经网络去预测这个回报。</p>
<p>在优化的过程中，梯度为让$V(s;\theta)$上升最快的方向。神经网络中是减去梯度，来梯度下降，这里是加上梯度来梯度上升。</p>
<h4 id="Actor-Critic方法"><a href="#Actor-Critic方法" class="headerlink" title="Actor-Critic方法"></a>Actor-Critic方法</h4><p>Actor-Critic方法结合了基于价值学习和基于策略学习两者，它有一个actor作为策略网络，有一个critic作为价值网络：</p>
<ul>
<li>actor：用神经网络$\pi(a|s ; \theta)$去近似$\pi(a|s)$。</li>
<li>critic：用神经网络$q(s,a;\omega)$去近似$Q_\pi(s,a)$</li>
</ul>
<p>在训练过程中两者是一起训练的，训练的过程如下：</p>
<ol>
<li>观察到状态$s_t$</li>
<li>actor根据自己的$\pi(a|s ; \theta)$网络给出的概率，（按概率）随机选一个动作执行。</li>
<li>观察到下一个状态$s_{t+1}$并得到奖励$r_t$</li>
<li>用TD学习更新critic网络中的参数$\omega$</li>
<li>用策略梯度（需要critic预测回报），更新critic网络中的$\theta$</li>
</ol>
<p>在训练全部完成后，以后做出决策就只需要策略网络（actor），而不需要critic。</p>
<h3 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h3><p>AlphaGo的训练有三步：</p>
<ol>
<li>通过behavior cloning初始化策略网络</li>
<li>通过策略梯度训练策略网络。</li>
<li>训练完策略网络后，用策略网络去训练一个价值网络。</li>
</ol>
<p>最后通过蒙特卡洛树搜索的方法使用策略网络和价值网络。</p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><h5 id="behavior-cloning"><a href="#behavior-cloning" class="headerlink" title="behavior cloning"></a>behavior cloning</h5><p>因为如果直接进行强化学习，速度太慢。所以首先根据人类已有的棋局资料，去训练策略网络。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240529171455797.png" alt="image-20240529171455797"></p>
<h5 id="训练策略网络"><a href="#训练策略网络" class="headerlink" title="训练策略网络"></a>训练策略网络</h5><p>因为仅仅用behavior cloning还会有很多状况考虑不到。所以还需要训练策略网络。在训练策略网络的过程中，通过跑完全程得到回报期望。即如果最终赢了，这局所有的行为的分数都+1；如果平局则这句行为的分数不变；如果最终输了，这局所有行为的分数都-1。</p>
<h5 id="训练价值网络"><a href="#训练价值网络" class="headerlink" title="训练价值网络"></a>训练价值网络</h5><p>仅用策略网络，有可能出现在某一步走错之后步步错。所以还需要用策略网络训练一个价值网络（与之前不同，这里是拟合状态价值函数）。之后就可以结合策略网络和价值网络进行决策。</p>
<h4 id="运行过程"><a href="#运行过程" class="headerlink" title="运行过程"></a>运行过程</h4><p>运行的过程用到了蒙特卡洛树搜索的方法。其有四步：</p>
<ul>
<li><p>Selection：</p>
<ul>
<li><p>设当前状态为$s_t$</p>
</li>
<li><p>对所有有效的行为$a$，计算行为$a$的得分：</p>
<script type="math/tex; mode=display">
\operatorname{score}(a)=Q(a)+\eta * \frac{\pi\left(a | s_{t} ; \theta\right)}{1+N(a)}</script><p>其中$Q(a)$是蒙特卡洛树计算的行为得分，$\pi\left(a | s_{t}\right)$是策略网络给出的行为概率，$N(a)$是到目前为止一共选择了行为$a$​多少次。</p>
</li>
<li><p>选择具有最大$\operatorname{score}(a)$的行为$a_t$​。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240529191318821.png" alt="image-20240529191318821"></p>
</li>
</ul>
</li>
<li><p>Expansion：</p>
<ul>
<li><p>选择了$a_t$行为，所以到达了$s_{t+1}$的状态。</p>
</li>
<li><p>然后通过策略网络，不断自我博弈，直到走到游戏结束</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240529191348057.png" alt="image-20240529191348057"></p>
</li>
</ul>
</li>
<li><p>Evaluation：</p>
<ul>
<li>计算：<script type="math/tex; mode=display">
V(s_{t+1})=\frac{1}{2} v\left(s_{t+1} ; w\right)+\frac{1}{2} r_{T}</script>其中$v\left(s_{\mathrm{t}+1} ; {w}\right)$是价值网络的评分；如果赢了，则$r_T=1$，如果输了，则$r_T=-1$。</li>
</ul>
</li>
<li><p>Backup：</p>
<ul>
<li><p>重复上面三步，每一个$a_t$都会记录一系列的$V_i$​值</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240529191705228.png" alt="image-20240529191705228"></p>
</li>
<li><p>更新</p>
<script type="math/tex; mode=display">
\left.Q\left(a_{\mathrm{t}}\right)=\text { mean(the recorded } V^{\prime} s\right)</script></li>
</ul>
</li>
</ul>
<p>最后，选择被蒙特卡洛树搜索选择最多次的$a_t$作为下一个行为。</p>
<h1 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h1><p>大多数机器学习和数据挖掘技术可能对高维数据无效。并且高维的数据会比较稀疏，有可能导致数据“过于”可分，导致过拟合。而且高维度中用距离来衡量样本相似性的方法会渐渐失效。</p>
<p>数据降维不仅可以帮助可视化，还可以进行数据压缩和噪声过滤。</p>
<p>数据降维的方法主要有特征选择与特征降维。它们的区别如下：</p>
<ol>
<li>特征选择：减少特征的数量，只使用选中的特征。</li>
<li>特征降维：所有原始特征都用于计算，但最终模型使用的是这些原始特征的线性或非线性组合，以降低数据的维度</li>
</ol>
<p>特征降维的方法有：</p>
<ul>
<li>无监督学习：<ul>
<li>截断奇异值分解（Truncated Singular Value Decomposition, SVD）</li>
<li>独立成分分析（Independent Component Analysis, ICA）</li>
<li>主成分分析（Principal Component Analysis, PCA）</li>
<li>局部线性嵌入（Locally Linear Embedding, LLE）</li>
<li>拉普拉斯特征映射（Laplacian Eigenmaps, LE）</li>
</ul>
</li>
<li>有监督学习：<ul>
<li>线性判别分析（Linear Discriminant Analysis, LDA）</li>
<li>典型相关分析（Canonical Correlation Analysis, CCA）</li>
</ul>
</li>
</ul>
<h2 id="PCA主成分分析"><a href="#PCA主成分分析" class="headerlink" title="PCA主成分分析"></a>PCA主成分分析</h2><p>主成分分析（PCA）是一种常用于降维的数据分析技术，通过<strong>将原始高维数据投影到新的低维空间来保留数据中的主要信息</strong>。以下是PCA的算法流程：</p>
<ol>
<li><p><strong>数据标准化</strong>：将数据标准化为均值为0，方差为1的形式。这一步是为了消除不同特征之间量纲不一致的影响。</p>
<script type="math/tex; mode=display">
\text{Standardized data} = \frac{X - \mu}{\sigma}</script></li>
<li><p><strong>计算协方差矩阵</strong>：计算标准化数据的协方差矩阵，以了解特征之间的相关性。</p>
<script type="math/tex; mode=display">
\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}</script><p>其中，$\mathbf{X}$ 是标准化后的数据矩阵， $n$​是样本数量。</p>
<blockquote>
<p>如果两个变量的协方差为正，则表示一个变量的增加通常与另一个变量的增加相联系；如果协方差为负，则表示一个变量的增加通常与另一个变量的减少相联系。</p>
</blockquote>
</li>
<li><p><strong>特征值分解</strong>：对协方差矩阵进行特征值分解，得到特征值和特征向量。</p>
<script type="math/tex; mode=display">
\mathbf{C} \mathbf{v} = \lambda \mathbf{v}</script><p>其中，$\lambda$ 为特征值，$\mathbf{v}$ 为对应的特征向量。</p>
</li>
<li><p><strong>选择主成分</strong>：根据特征值的大小选择前k个最大的特征值对应的特征向量，作为主成分。</p>
</li>
<li><p><strong>转换数据</strong>：将原始数据投影到选定的特征向量空间，得到降维后的数据。</p>
<script type="math/tex; mode=display">
\mathbf{X}_{\text{new}} = \mathbf{X} \mathbf{W}</script><p>其中， $\mathbf{W}$ 是由选定的特征向量构成的矩阵。</p>
</li>
</ol>
<h3 id="示例：应用PCA进行降维"><a href="#示例：应用PCA进行降维" class="headerlink" title="示例：应用PCA进行降维"></a>示例：应用PCA进行降维</h3><p>假设我们有一个三维数据集如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征1</th>
<th>特征2</th>
<th>特征3</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.5</td>
<td>2.4</td>
<td>3.5</td>
</tr>
<tr>
<td>0.5</td>
<td>0.7</td>
<td>1.2</td>
</tr>
<tr>
<td>2.2</td>
<td>2.9</td>
<td>3.1</td>
</tr>
<tr>
<td>1.9</td>
<td>2.2</td>
<td>2.9</td>
</tr>
<tr>
<td>3.1</td>
<td>3.0</td>
<td>3.6</td>
</tr>
<tr>
<td>2.3</td>
<td>2.7</td>
<td>3.4</td>
</tr>
<tr>
<td>2.0</td>
<td>1.6</td>
<td>2.7</td>
</tr>
<tr>
<td>1.0</td>
<td>1.1</td>
<td>1.2</td>
</tr>
<tr>
<td>1.5</td>
<td>1.6</td>
<td>1.9</td>
</tr>
<tr>
<td>1.1</td>
<td>0.9</td>
<td>1.5</td>
</tr>
</tbody>
</table>
</div>
<h4 id="1-数据标准化"><a href="#1-数据标准化" class="headerlink" title="1. 数据标准化"></a>1. 数据标准化</h4><p>将每个特征标准化，标准化后的数据：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
0.926 & 0.610 & 1.096 \\
-1.759 & -1.507 & -1.425 \\
0.524 & 1.233 & 0.658 \\
0.121 & 0.361 & 0.439 \\
1.732 & 1.357 & 1.206 \\
0.658 & 0.984 & 0.987 \\
0.255 & -0.386 & 0.219 \\
-1.087 & -1.009 & -1.425 \\
-0.416 & -0.386 & -0.658 \\
-0.953 & -1.258 & -1.096 \\
\end{bmatrix}</script><h4 id="2-计算协方差矩阵"><a href="#2-计算协方差矩阵" class="headerlink" title="2. 计算协方差矩阵"></a>2. 计算协方差矩阵</h4><p>计算标准化数据的协方差矩阵：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
1.111 & 1.029 & 1.065 \\
1.029 & 1.111 & 1.042 \\
1.065 & 1.042 & 1.111 \\
\end{bmatrix}</script><h4 id="3-特征值分解"><a href="#3-特征值分解" class="headerlink" title="3. 特征值分解"></a>3. 特征值分解</h4><p>对协方差矩阵进行特征值分解，得到特征值和特征向量。</p>
<p>特征值：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
3.201, & 0.045, & 0.087
\end{bmatrix}</script><p>特征向量：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
-0.578 & -0.619 & 0.531 \\
-0.574 & -0.155 & -0.804 \\
-0.580 & 0.770 & 0.266 \\
\end{bmatrix}</script><h4 id="4-选择主成分"><a href="#4-选择主成分" class="headerlink" title="4. 选择主成分"></a>4. 选择主成分</h4><p>选择前两个最大的特征值对应的特征向量作为主成分，则选择的特征向量：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
-0.578 & 0.531 \\
-0.574 & -0.804 \\
-0.580 & 0.266 \\
\end{bmatrix}</script><h4 id="5-转换数据"><a href="#5-转换数据" class="headerlink" title="5. 转换数据"></a>5. 转换数据</h4><p>将原始数据投影到选定的特征向量空间，转换后的数据：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
-1.522 & 0.293 \\
2.708 & -0.102 \\
-1.392 & -0.538 \\
-0.532 & -0.110 \\
-2.479 & 0.149 \\
-1.517 & -0.179 \\
-0.053 & 0.504 \\
2.034 & -0.146 \\
0.844 & -0.086 \\
1.909 & 0.213 \\
\end{bmatrix}</script><p>这样，我们将原始的三维数据降维成了二维数据，同时保留了数据中的主要信息。</p>
<h2 id="SVD（奇异值分解）"><a href="#SVD（奇异值分解）" class="headerlink" title="SVD（奇异值分解）"></a>SVD（奇异值分解）</h2><p>SVD是另一种数据降维的技术。算法流程如下：</p>
<ol>
<li><p><strong>构建矩阵</strong>：构建一个数据矩阵 $A$，其中包含要处理的所有数据。</p>
</li>
<li><p><strong>奇异值分解</strong>：对矩阵 $A$ 进行奇异值分解，得到三个矩阵 $U$、$\Sigma$ 和 $V^T$。</p>
<script type="math/tex; mode=display">
A = U \Sigma V^T</script><p>其中，$ U $是$ m \times m $的正交矩阵，$ \Sigma $是$ m \times n $的对角矩阵，$ V^T $是$ n \times n $​ 的正交矩阵。</p>
</li>
<li><p><strong>选择主成分</strong>：根据奇异值的大小选择前k个最大的奇异值，构建低秩近似。</p>
</li>
<li><p><strong>重构矩阵</strong>：利用选择的奇异值和对应的左右奇异向量重构原始矩阵的低秩近似。</p>
<script type="math/tex; mode=display">
A_k = U_k \Sigma_k V_k^T</script><p>其中，$ U_k $和$ V_k $分别是$ U $和$ V $的前 k 列，$ \Sigma_k $是$ \Sigma $ 的前 k 个奇异值构成的对角矩阵。</p>
</li>
</ol>
<p>给定一个向量$q$，可以通过$qV$得到它属于哪一个类。</p>
<h3 id="示例：Latent-Semantic-Indexing（潜在语义索引）"><a href="#示例：Latent-Semantic-Indexing（潜在语义索引）" class="headerlink" title="示例：Latent Semantic Indexing（潜在语义索引）"></a>示例：Latent Semantic Indexing（潜在语义索引）</h3><div class="table-container">
<table>
<thead>
<tr>
<th>document\term</th>
<th>data</th>
<th>information</th>
<th>retrieval</th>
<th>brain</th>
<th>lung</th>
</tr>
</thead>
<tbody>
<tr>
<td>CS-TR1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>CS-TR2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>CS-TR3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>CS-TR4</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>MED-TR1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>MED-TR2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>MED-TR3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>假设有这样的一个数据集，其中第一列是文档类型（CS代表计算机，MED代表医学）。可以看到data、information、retrieval这些词只在计算机类的文档中出现；而brain、lung只在医学类的文档中出现。</p>
<h4 id="1-构建矩阵"><a href="#1-构建矩阵" class="headerlink" title="1. 构建矩阵"></a>1. 构建矩阵</h4><p>构建文档-词项矩阵  $A$ ：</p>
<script type="math/tex; mode=display">
A = \begin{bmatrix}
1 & 1 & 1 & 0 & 0 \\
2 & 2 & 2 & 0 & 0 \\
1 & 1 & 1 & 0 & 0 \\
5 & 5 & 5 & 0 & 0 \\
0 & 0 & 0 & 2 & 2 \\
0 & 0 & 0 & 3 & 3 \\
0 & 0 & 0 & 1 & 1 \\
\end{bmatrix}</script><h4 id="2-奇异值分解"><a href="#2-奇异值分解" class="headerlink" title="2. 奇异值分解"></a>2. 奇异值分解</h4><p>对矩阵$A$进行奇异值分解，可以得到三个矩阵为：</p>
<script type="math/tex; mode=display">
U \Sigma V^T=\left[\begin{array}{ll}
0.18 & 0 \\
0.36 & 0 \\
0.18 & 0 \\
0.90 & 0 \\
0 & 0.53 \\
0 & 0.80 \\
0 & 0.27
\end{array}\right]\times\left[\begin{array}{ll}
9.64 & 0 \\
0 & 5.29
\end{array}\right]\times\left[\begin{array}{ll}
0.58 & 0.58 & 0.58 & 0 & 0 \\
0 & 0 & 0 & 0.71 & 0.71
\end{array}\right]</script><p>在这里其实就可以发现矩阵$U$前三行第一列不为0的刚好对应的是CS类的。而第二列不为0的刚好对应的是MED类的。而$V$也同理。</p>
<h4 id="3-选择主成分"><a href="#3-选择主成分" class="headerlink" title="3. 选择主成分"></a>3. 选择主成分</h4><p>这里可以看到为9.64的奇异值较大，所以就选择其。</p>
<h4 id="4-重构矩阵"><a href="#4-重构矩阵" class="headerlink" title="4. 重构矩阵"></a>4. 重构矩阵</h4><p>然后将$\Sigma$矩阵中的没有选择的奇异值改为0。也就是：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
9.64 & 0 \\
0 & 5.29
\end{array}\right]\rightarrow\left[\begin{array}{ll}
9.64 & 0 \\
0 & 0
\end{array}\right]</script><p>然后再重新计算$A_k$。</p>
<script type="math/tex; mode=display">
A_k=\left[\begin{array}{ll}
0.18 & 0 \\
0.36 & 0 \\
0.18 & 0 \\
0.90 & 0 \\
0 & 0.53 \\
0 & 0.80 \\
0 & 0.27
\end{array}\right]\times\left[\begin{array}{ll}
9.64 & 0 \\
0 & 0
\end{array}\right]\times\left[\begin{array}{ll}
0.58 & 0.58 & 0.58 & 0 & 0 \\
0 & 0 & 0 & 0.71 & 0.71
\end{array}\right]</script><p>得到低秩近似$A_k$：</p>
<script type="math/tex; mode=display">
A_k=\left[\begin{array}{lllll}
1 & 1 & 1 & 0 & 0 \\
2 & 2 & 2 & 0 & 0 \\
1 & 1 & 1 & 0 & 0 \\
5 & 5 & 5 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]</script><h2 id="LDA（线性判别法）"><a href="#LDA（线性判别法）" class="headerlink" title="LDA（线性判别法）"></a>LDA（线性判别法）</h2><p>LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/人工智能.assets/image-20240612113559022.png" alt="image-20240612113559022"></p>
<p>参考文章：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html</a></p>
<h2 id="LLE（局部线性嵌入）"><a href="#LLE（局部线性嵌入）" class="headerlink" title="LLE（局部线性嵌入）"></a>LLE（局部线性嵌入）</h2><p>LLE(Locally Linear Embedding)优势在于降维时保持样本局部的线性特征。它广泛的用于图像图像识别，高维数据可视化等领域。</p>
<h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>LLE首先假设数据在较小的局部是线性的。即某一个数据可以由它邻域中的几个样本来线性表示。</p>
<p>比如有一个样本 $x_1$，在它的原始高维邻域里用K-近邻思想找到和它最近的三个样本 $x_2, x_3, x_4$。然后假设 $x_1$ 可以由 $x_2, x_3, x_4$ 线性表示，即：</p>
<script type="math/tex; mode=display">
x_1 = w_{12}x_2 + w_{13}x_3 + w_{14}x_4</script><p>其中，$w_{12}, w_{13}, w_{14}$ 为权重系数。在通过LLE降维后，希望 $x_1$ 在低维空间对应的投影 $x’_1$ 和 $x_2, x_3, x_4$ 对应的投影 $x’_2, x’_3, x’_4$ 也尽量保持同样的线性关系，即</p>
<script type="math/tex; mode=display">
x'_1 \approx w_{12}x'_2 + w_{13}x'_3 + w_{14}x'_4</script><p>也就是说，投影前后线性关系的权重系数 $w_{12}, w_{13}, w_{14}$ 是尽量不变或者最小改变的。</p>
<p>从上面可以看出，线性关系只在样本的附近起作用，离样本远的样本对局部的线性关系没有影响，因此降维的复杂度降低了很多。</p>
<p>参考文章：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6266408.html">https://www.cnblogs.com/pinard/p/6266408.html</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ran.ranruo.xyz">白</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ran.ranruo.xyz/post/3254563453.html">https://ran.ranruo.xyz/post/3254563453.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ran.ranruo.xyz" target="_blank">白のBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a></div><div class="post_share"><div class="social-share" data-image="/img/head_protrait.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/2370170588.html" title="体素、网格与点云"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">体素、网格与点云</div></div></a></div><div class="next-post pull-right"><a href="/post/554039588.html" title="Macports常用指令"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Macports常用指令</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head_protrait.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">白</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E4%B8%89%E5%A4%A7%E5%AD%A6%E6%B4%BE"><span class="toc-number">1.</span> <span class="toc-text">人工智能的三大学派</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%B9%8B%E6%A0%B9%EF%BC%9A%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.</span> <span class="toc-text">智能之根：数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AE%A4%E8%AF%86"><span class="toc-number">2.2.</span> <span class="toc-text">数据的认识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.3.</span> <span class="toc-text">数据集的类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F"><span class="toc-number">2.4.</span> <span class="toc-text">数据质量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF"><span class="toc-number">2.5.</span> <span class="toc-text">数据预处理技术</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD%E7%9A%84%E4%B8%8D%E5%90%8C%E7%90%86%E8%A7%A3%E6%96%B9%E5%BC%8F"><span class="toc-number">2.6.</span> <span class="toc-text">从数据到智能的不同理解方式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E7%9A%84%E8%A1%A8%E7%A4%BA%E4%B8%8E%E6%8E%A8%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">知识的表示与推理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E4%BB%A3%E9%80%BB%E8%BE%91%E5%AD%A6"><span class="toc-number">3.1.</span> <span class="toc-text">现代逻辑学</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E9%A2%98%E9%80%BB%E8%BE%91"><span class="toc-number">3.1.1.</span> <span class="toc-text">命题逻辑</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E7%BB%93%E6%8E%A8%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">归结推理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AD%90%E5%8F%A5"><span class="toc-number">3.2.1.</span> <span class="toc-text">1. 子句</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">特点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%B0%93%E8%AF%8D%E5%85%AC%E5%BC%8F%E8%BD%AC%E5%AD%90%E5%8F%A5%E9%9B%86"><span class="toc-number">3.2.2.</span> <span class="toc-text">2. 谓词公式转子句集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BD%92%E7%BB%93%E5%BC%8F"><span class="toc-number">3.2.3.</span> <span class="toc-text">3. 归结式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E7%BB%93%E5%BC%8F%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.2.3.1.</span> <span class="toc-text">归结式的表示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%BD%92%E7%BB%93%E6%8E%A8%E5%AF%BC"><span class="toc-number">3.2.4.</span> <span class="toc-text">4. 归结推导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%91%BD%E9%A2%98%E9%80%BB%E8%BE%91-1"><span class="toc-number">3.2.4.1.</span> <span class="toc-text">命题逻辑</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%93%E8%AF%8D%E9%80%BB%E8%BE%91"><span class="toc-number">3.2.4.2.</span> <span class="toc-text">谓词逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BD%AE%E6%8D%A2%E5%92%8C%E5%90%88%E4%B8%80"><span class="toc-number">3.2.4.2.1.</span> <span class="toc-text">置换和合一</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E4%B8%80%E8%88%AC%E5%90%88%E4%B8%80%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.4.2.2.</span> <span class="toc-text">最一般合一算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%BD%92%E7%BB%93%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.2.5.</span> <span class="toc-text">5. 归结实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%BD%92%E7%BB%93%E7%AD%96%E7%95%A5"><span class="toc-number">3.2.6.</span> <span class="toc-text">6. 归结策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1"><span class="toc-number">3.3.</span> <span class="toc-text">知识图谱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86%EF%BC%9AFOIL%E7%AE%97%E6%B3%95"><span class="toc-number">3.3.1.</span> <span class="toc-text">知识图谱推理：FOIL算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AF"><span class="toc-number">4.</span> <span class="toc-text">搜索技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B2%E7%9B%AE%E6%90%9C%E7%B4%A2"><span class="toc-number">4.1.</span> <span class="toc-text">盲目搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.1.</span> <span class="toc-text">常用的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%AF%E5%BE%84%E6%A3%80%E6%B5%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">路径检测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8F%91%E6%90%9C%E7%B4%A2"><span class="toc-number">4.2.</span> <span class="toc-text">启发搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A%E6%90%9C%E7%B4%A2"><span class="toc-number">4.2.1.</span> <span class="toc-text">A搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8F%91%E5%87%BD%E6%95%B0%E7%9A%84%E8%A6%81%E6%B1%82"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">启发函数的要求</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E6%90%9C%E7%B4%A2"><span class="toc-number">4.2.2.</span> <span class="toc-text">A*搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#IDA-%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.3.</span> <span class="toc-text">IDA*算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%BE%E5%BC%9B%E9%97%AE%E9%A2%98"><span class="toc-number">4.2.4.</span> <span class="toc-text">松弛问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%8A%97%E6%80%A7%E6%90%9C%E7%B4%A2"><span class="toc-number">4.3.</span> <span class="toc-text">对抗性搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MinMax%E7%AD%96%E7%95%A5"><span class="toc-number">4.3.1.</span> <span class="toc-text">MinMax策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Alpha-Beta%E5%89%AA%E6%9E%9D"><span class="toc-number">4.3.2.</span> <span class="toc-text">Alpha-Beta剪枝</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2"><span class="toc-number">4.4.</span> <span class="toc-text">高级搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%B1%B1%E7%AE%97%E6%B3%95"><span class="toc-number">4.4.1.</span> <span class="toc-text">爬山算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95"><span class="toc-number">4.4.2.</span> <span class="toc-text">模拟退火算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><span class="toc-number">4.4.2.1.</span> <span class="toc-text">基本要素</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">4.4.2.2.</span> <span class="toc-text">基本流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="toc-number">4.4.3.</span> <span class="toc-text">遗传算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B-1"><span class="toc-number">4.4.3.1.</span> <span class="toc-text">基本流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F"><span class="toc-number">4.4.3.2.</span> <span class="toc-text">编码方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%82%E5%BA%94%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="toc-number">4.4.3.3.</span> <span class="toc-text">适应度计算方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%AD%90%E4%BB%A3%E6%96%B9%E5%BC%8F"><span class="toc-number">4.4.3.4.</span> <span class="toc-text">选择子代方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E6%96%B9%E5%BC%8F"><span class="toc-number">4.4.3.5.</span> <span class="toc-text">交叉方式</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">5.</span> <span class="toc-text">贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">贝叶斯定理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">5.2.</span> <span class="toc-text">朴素贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Laplace-%E5%B9%B3%E6%BB%91"><span class="toc-number">5.2.1.</span> <span class="toc-text">Laplace 平滑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B"><span class="toc-number">5.2.2.</span> <span class="toc-text">分类过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">5.3.</span> <span class="toc-text">贝叶斯网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%B1%BB%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B"><span class="toc-number">5.3.1.</span> <span class="toc-text">三类条件独立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A0%E9%80%92"><span class="toc-number">5.3.1.1.</span> <span class="toc-text">传递</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B1%E5%9B%A0"><span class="toc-number">5.3.1.2.</span> <span class="toc-text">共因</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B1%E6%9E%9C"><span class="toc-number">5.3.1.3.</span> <span class="toc-text">共果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B-1"><span class="toc-number">5.3.2.</span> <span class="toc-text">分类过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E9%87%8F%E6%B6%88%E9%99%A4"><span class="toc-number">5.3.2.1.</span> <span class="toc-text">变量消除</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E5%9B%A0%E5%AD%90"><span class="toc-number">5.3.2.1.1.</span> <span class="toc-text">合并因子</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B6%88%E9%99%A4%E5%9B%A0%E5%AD%90"><span class="toc-number">5.3.2.1.2.</span> <span class="toc-text">消除因子</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B7%B2%E7%9F%A5%E5%8F%96%E5%80%BC%E5%8F%98%E9%87%8F"><span class="toc-number">5.3.2.1.3.</span> <span class="toc-text">已知取值变量</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.</span> <span class="toc-text">机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">6.1.</span> <span class="toc-text">机器学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means%E7%AE%97%E6%B3%95"><span class="toc-number">6.2.1.</span> <span class="toc-text">K-means算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">算法评价方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#K-means%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="toc-number">6.2.1.2.</span> <span class="toc-text">K-means算法的改进</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#K-means"><span class="toc-number">6.2.1.2.1.</span> <span class="toc-text">K-means++</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#K-means-1"><span class="toc-number">6.2.1.2.2.</span> <span class="toc-text">K-means||</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DBSCAN"><span class="toc-number">6.2.2.</span> <span class="toc-text">DBSCAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.3.</span> <span class="toc-text">评价方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%B1%E8%81%9A%E7%B1%BB"><span class="toc-number">6.2.4.</span> <span class="toc-text">谱聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5"><span class="toc-number">6.2.4.1.</span> <span class="toc-text">图的一些概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E5%88%86%E5%89%B2"><span class="toc-number">6.2.4.2.</span> <span class="toc-text">图分割</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%A0%E5%90%91%E5%9B%BE%E5%88%87%E5%9B%BE"><span class="toc-number">6.2.4.2.1.</span> <span class="toc-text">无向图切图</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Cut"><span class="toc-number">6.2.4.2.1.1.</span> <span class="toc-text">Cut</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#RatioCut"><span class="toc-number">6.2.4.2.1.2.</span> <span class="toc-text">RatioCut</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#NCut"><span class="toc-number">6.2.4.2.1.3.</span> <span class="toc-text">NCut</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">6.2.4.3.</span> <span class="toc-text">算法步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%9B%BE"><span class="toc-number">6.2.4.3.1.</span> <span class="toc-text">构建图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5"><span class="toc-number">6.2.4.3.2.</span> <span class="toc-text">构建拉普拉斯矩阵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E5%88%87%E5%9B%BE%E6%96%B9%E6%B3%95%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E5%80%BC"><span class="toc-number">6.2.4.3.3.</span> <span class="toc-text">根据切图方法计算特征值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9F%A9%E9%98%B5"><span class="toc-number">6.2.4.3.4.</span> <span class="toc-text">计算特征矩阵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E7%89%B9%E5%BE%81%E7%9F%A9%E9%98%B5%E8%81%9A%E7%B1%BB"><span class="toc-number">6.2.4.3.5.</span> <span class="toc-text">对特征矩阵聚类</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM-%E4%B8%8D%E8%80%83"><span class="toc-number">6.3.</span> <span class="toc-text">SVM(不考)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">6.3.1.</span> <span class="toc-text">基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9"><span class="toc-number">6.3.1.1.</span> <span class="toc-text">结构风险</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94"><span class="toc-number">6.3.1.2.</span> <span class="toc-text">几何间隔</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">6.3.2.</span> <span class="toc-text">SVM目标函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E9%97%AE%E9%A2%98"><span class="toc-number">6.3.2.1.</span> <span class="toc-text">原问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">6.3.2.2.</span> <span class="toc-text">对偶问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%8F%98%E6%8D%A2"><span class="toc-number">6.3.2.2.1.</span> <span class="toc-text">拉格朗日变换</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B1%82%E8%A7%A3%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">6.3.2.2.2.</span> <span class="toc-text">求解对偶问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">6.3.3.</span> <span class="toc-text">核函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B"><span class="toc-number">6.3.3.1.</span> <span class="toc-text">SVM分类过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">6.3.3.2.</span> <span class="toc-text">核函数的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">6.3.3.3.</span> <span class="toc-text">核函数的定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">6.3.3.4.</span> <span class="toc-text">常见的核函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F%E5%A4%84%E7%90%86%E7%A6%BB%E7%BE%A4%E7%82%B9"><span class="toc-number">6.3.4.</span> <span class="toc-text">松弛变量处理离群点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.4.</span> <span class="toc-text">人工神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E5%8F%B2"><span class="toc-number">6.4.1.</span> <span class="toc-text">发展史</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MP%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.4.1.1.</span> <span class="toc-text">MP模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">6.4.1.2.</span> <span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">6.4.1.3.</span> <span class="toc-text">多层感知机</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">6.4.2.</span> <span class="toc-text">神经网络基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.4.3.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">6.4.3.1.</span> <span class="toc-text">卷积操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%BB%B4%E5%BA%A6%E5%85%AC%E5%BC%8F"><span class="toc-number">6.4.3.1.1.</span> <span class="toc-text">卷积维度公式</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">6.4.3.2.</span> <span class="toc-text">池化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.4.4.</span> <span class="toc-text">循环神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89"><span class="toc-number">6.4.5.</span> <span class="toc-text">长短期记忆网络（LSTM）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.5.</span> <span class="toc-text">强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">6.5.1.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%80%A7%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%9D%A5%E6%BA%90"><span class="toc-number">6.5.1.1.</span> <span class="toc-text">随机性的两个来源</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AI%E7%8E%A9%E6%B8%B8%E6%88%8F"><span class="toc-number">6.5.1.2.</span> <span class="toc-text">AI玩游戏</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E6%8A%A5"><span class="toc-number">6.5.1.3.</span> <span class="toc-text">回报</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">6.5.1.4.</span> <span class="toc-text">价值函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%8C%E4%B8%BA%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">6.5.1.4.1.</span> <span class="toc-text">行为价值函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">6.5.1.4.2.</span> <span class="toc-text">状态价值函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E8%A1%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.5.2.</span> <span class="toc-text">进行强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.5.2.1.</span> <span class="toc-text">基于价值学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.5.2.2.</span> <span class="toc-text">基于策略学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Actor-Critic%E6%96%B9%E6%B3%95"><span class="toc-number">6.5.2.3.</span> <span class="toc-text">Actor-Critic方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AlphaGo"><span class="toc-number">6.5.3.</span> <span class="toc-text">AlphaGo</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">6.5.3.1.</span> <span class="toc-text">训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#behavior-cloning"><span class="toc-number">6.5.3.1.1.</span> <span class="toc-text">behavior cloning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C"><span class="toc-number">6.5.3.1.2.</span> <span class="toc-text">训练策略网络</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%BB%B7%E5%80%BC%E7%BD%91%E7%BB%9C"><span class="toc-number">6.5.3.1.3.</span> <span class="toc-text">训练价值网络</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">6.5.3.2.</span> <span class="toc-text">运行过程</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4"><span class="toc-number">7.</span> <span class="toc-text">数据降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">7.1.</span> <span class="toc-text">PCA主成分分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%BA%94%E7%94%A8PCA%E8%BF%9B%E8%A1%8C%E9%99%8D%E7%BB%B4"><span class="toc-number">7.1.1.</span> <span class="toc-text">示例：应用PCA进行降维</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">7.1.1.1.</span> <span class="toc-text">1. 数据标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%AE%A1%E7%AE%97%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5"><span class="toc-number">7.1.1.2.</span> <span class="toc-text">2. 计算协方差矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">7.1.1.3.</span> <span class="toc-text">3. 特征值分解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E9%80%89%E6%8B%A9%E4%B8%BB%E6%88%90%E5%88%86"><span class="toc-number">7.1.1.4.</span> <span class="toc-text">4. 选择主成分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span class="toc-number">7.1.1.5.</span> <span class="toc-text">5. 转换数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVD%EF%BC%88%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%89"><span class="toc-number">7.2.</span> <span class="toc-text">SVD（奇异值分解）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9ALatent-Semantic-Indexing%EF%BC%88%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95%EF%BC%89"><span class="toc-number">7.2.1.</span> <span class="toc-text">示例：Latent Semantic Indexing（潜在语义索引）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9E%84%E5%BB%BA%E7%9F%A9%E9%98%B5"><span class="toc-number">7.2.1.1.</span> <span class="toc-text">1. 构建矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">7.2.1.2.</span> <span class="toc-text">2. 奇异值分解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%80%89%E6%8B%A9%E4%B8%BB%E6%88%90%E5%88%86"><span class="toc-number">7.2.1.3.</span> <span class="toc-text">3. 选择主成分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E9%87%8D%E6%9E%84%E7%9F%A9%E9%98%B5"><span class="toc-number">7.2.1.4.</span> <span class="toc-text">4. 重构矩阵</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA%EF%BC%88%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E6%B3%95%EF%BC%89"><span class="toc-number">7.3.</span> <span class="toc-text">LDA（线性判别法）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLE%EF%BC%88%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5%EF%BC%89"><span class="toc-number">7.4.</span> <span class="toc-text">LLE（局部线性嵌入）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">7.4.1.</span> <span class="toc-text">算法原理</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/158142455.html" title="用fast-reid提取人物特征">用fast-reid提取人物特征</a><time datetime="2025-04-08T14:52:00.000Z" title="发表于 2025-04-08 14:52:00">2025-04-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/670212549.html" title="博客搬家记">博客搬家记</a><time datetime="2025-04-01T11:50:26.000Z" title="发表于 2025-04-01 11:50:26">2025-04-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/284249815.html" title="图论">图论</a><time datetime="2024-09-19T18:00:11.000Z" title="发表于 2024-09-19 18:00:11">2024-09-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/1580825926.html" title="信号与系统">信号与系统</a><time datetime="2024-09-19T18:00:10.000Z" title="发表于 2024-09-19 18:00:10">2024-09-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/2529764766.html" title="从0开始配置mindtorch环境">从0开始配置mindtorch环境</a><time datetime="2024-09-19T17:00:10.000Z" title="发表于 2024-09-19 17:00:10">2024-09-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 白</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>